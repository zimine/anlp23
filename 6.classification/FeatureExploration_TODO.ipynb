{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores feature engineering for text classification.  Your task is to create two new feature functions (like `dictionary_feature` and `unigram_feature` below), and include them in the `build_features` function.  A check grade will be given to generic features that apply across arbitrary text classification problems (e.g., a feature for bigrams); check+ will be given for at least one feature that reveals your own understanding of the data. What features do you think will help for your particular problem? Your grade is *not* tied to whether accuracy goes up or down, so be creative!  You are free to read in any other external resources you like (dictionaries, document metadata, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are free to use any of the following datasets for this exercise, or to use your own (if you have your own labeled data, I would encourage you to use it!).  If you use your own data, just be sure to format it like the examples below; each directory has a `train.tsv`, `dev.tsv` and `test.tsv` file, where each file is tab-separated (label in the first column and text in the second column).\n",
    "\n",
    "* [Sentiment Analysis](https://ai.stanford.edu/~amaas/data/sentiment/) (Positive/Negative): `data/lmrd`\n",
    "* [Congressional Speech](https://www.cs.cornell.edu/home/llee/data/convote.html) (Democrat/Republican): `data/convote`\n",
    "* Library of Congress Subject Classication ([21 categories](https://en.wikipedia.org/wiki/Library_of_Congress_Classification)): `data/loc`\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q0**: Briefly describe your data (including the categories you're predicting).  If you're using your own data, tell us about it; if you're using one of the datasets above, tell us something that shows you've looked at the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this homework, I'm focusing on the Large Movie Review Dataset `LMRD` for a binary classfication of positive and negative sentiments encoded in the reviews. Each subset (i.e. train, dev, and test) of the dataset consists of an equal number of pos/neg-tagged reviews. I'm a bit surprised to find out that the test set contains more reviews that the training subset. Altogether `LMRD` includes 25,000 movie reviews from IMDB. According to the original article, the authors include at most 30 reviews for a movie to make sure there's a proper range of movies included. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import Counter\n",
    "import operator\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "from math import sqrt \n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Count the number of reviews in each subset\n",
    "def count(data):\n",
    "    df = pd.read_csv(\"../data/lmrd/\"+data+\".tsv\", sep=\"\\t\", header=None)\n",
    "    tag1 = df.iloc[:,0].value_counts().index[0]\n",
    "    val1 = df.iloc[:,0].value_counts()[0]\n",
    "    tag2 = df.iloc[:,0].value_counts().index[1]\n",
    "    val2 = df.iloc[:,0].value_counts()[1]\n",
    "    print(f\"The {data} subset contains {val1} {tag1}tive reviews and {val2} {tag2}tive reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train subset contains 10000 postive reviews and 10000 negtive reviews.\n",
      "The test subset contains 12500 postive reviews and 12500 negtive reviews.\n",
      "The dev subset contains 2500 postive reviews and 2500 negtive reviews.\n"
     ]
    }
   ],
   "source": [
    "count(\"train\")\n",
    "count(\"test\")\n",
    "count(\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            label=cols[0]\n",
    "            text=word_tokenize(cols[1])\n",
    "            X.append(text)\n",
    "            Y.append(label)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Change this to the directory with the data you will be using.\n",
    "# The directory should contain train.tsv, dev.tsv and test.tsv\n",
    "directory=\"../data/lmrd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainX, trainY=read_data(\"%s/train.tsv\" % directory)\n",
    "devX, devY=read_data(\"%s/dev.tsv\" % directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def majority_class(trainY, devY):\n",
    "    labelCounts=Counter()\n",
    "    for label in trainY:\n",
    "        labelCounts[label]+=1\n",
    "    majority=labelCounts.most_common(1)[0][0]\n",
    "    \n",
    "    correct=0.\n",
    "    for label in devY:\n",
    "        if label == majority:\n",
    "            correct+=1\n",
    "            \n",
    "    print(\"%s\\t%.3f\" % (majority, correct/len(devY)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll create two feature classes -- one feature class noting the presence of a word in an external dictionary, and one feature class for the word identity (i.e., unigram).  We'll implement each feature class as a function that takes a single document as input (as a list of tokens) and returns a dict corresponding to the feature we're creating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start with a fixed dict to look up keywords occuring in the review\n",
    "pos_dictionary = set([\"like\", \"love\", \"good\"])\n",
    "neg_dictionary = set([\"dislike\", \"hate\", \"bad\"])\n",
    "\n",
    "def sentiment_dictionary_feature(tokens):\n",
    "    feats={}\n",
    "    for word in tokens:\n",
    "        if word in pos_dictionary:\n",
    "            feats[\"word_in_pos_dictionary\"]=1\n",
    "        if word in neg_dictionary:\n",
    "            feats[\"word_in_neg_dictionary\"]=1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add a unigram feature set\n",
    "def unigram_feature(tokens):\n",
    "    feats={}\n",
    "    for word in tokens:\n",
    "        feats[\"UNIGRAM_%s\" % word]=1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**: Add first new feature function here.  Describe your feature and why you think it will help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I continue with the unigram feature set and then create a bigram feature set to identify defining bigrams that can help distinguish between positive and negative-tagged movies. One potential advantage of using bigrams is to differentiate between phrases like `don't dislike` and `dislike`. The former contains a double negation, which can lead to misclassification as negative reviews when using unigrams alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add a bigram feature set\n",
    "def new_feature_class_one(tokens):\n",
    "    feats={}  \n",
    "    for i in range(len(tokens) - 1):\n",
    "        bigram = (tokens[i], tokens[i + 1])  \n",
    "        bigram = ' '.join(bigram)\n",
    "        feats[\"BIGRAM_%s\" % bigram] = 1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2**: Add second new feature function here. Describe your feature and why you think it will help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second feature set, I use a keyword dict to look up tokens in capital letter, which upon closer inspection of the data, occurs frequently in pos/neg-tagged reviews. These tokens stand out as they typically occur in all caps, expressing one's strong opinions towards the movie. For positive reviews, `WOW` and `ABSOLUTELTY` are among the most commonly all-cap tokens. For negative reviews, `NOT`, `BUT`, and the questions mark `?` occur frequently to express one's strong dissatisfaction with the movie. A unigram-only model treats all tokens as equal, which might not pick out all-cap tokens as mentioned above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add a keyword feature set\n",
    "pos_cap_dict = set([\"WOW\", \"ABSOLUTELY\", \"SO\", \"EVER\"])\n",
    "neg_cap_dict = set([\"NOT\", \"VERY\", \"BUT\", \"?\", \"ONLY\", \"NO\", \"WTF\", \"...\", \"SPENT\", \"$\"])\n",
    "\n",
    "def new_feature_class_two(tokens):\n",
    "    feats={}\n",
    "    for word in tokens:\n",
    "        if word in pos_cap_dict:\n",
    "            feats[\"allcaps_in_pos_dict\"]=1\n",
    "        if word in neg_cap_dict:\n",
    "            feats[\"allcaps_in_neg_dict\"]=1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main function we'll use to aggregate together all of the information from different feature classes.  Each document has a feature dict (`feats`), and we'll update that dict with the new dict that each separate feature class is returning.  (Here you want to make sure that the keys each feature function is creating are unique so they don't get clobbered by other functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_features(trainX, feature_functions):\n",
    "    data=[]\n",
    "    for tokens in trainX:\n",
    "        feats={}\n",
    "\n",
    "        for function in feature_functions:\n",
    "            feats.update(function(tokens))\n",
    "\n",
    "        data.append(feats)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This helper function converts a dictionary of feature names to unique numerical ids\n",
    "def create_vocab(data):\n",
    "    feature_vocab={}\n",
    "    idx=0\n",
    "    for doc in data:\n",
    "        for feat in doc:\n",
    "            if feat not in feature_vocab:\n",
    "                feature_vocab[feat]=idx\n",
    "                idx+=1\n",
    "                \n",
    "    return feature_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This helper function converts a dictionary of feature names to a sparse representation\n",
    "# that we can fit in a scikit-learn model.  This is important because almost all feature \n",
    "# values will be 0 for most documents (note: why?), and we don't want to save them all in \n",
    "# memory.\n",
    "\n",
    "def features_to_ids(data, feature_vocab):\n",
    "    new_data=sparse.lil_matrix((len(data), len(feature_vocab)))\n",
    "    for idx,doc in enumerate(data):\n",
    "        for f in doc:\n",
    "            if f in feature_vocab:\n",
    "                new_data[idx,feature_vocab[f]]=doc[f]\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function evaluates a list of feature functions on the training/dev data arguments\n",
    "def pipeline(trainX, devX, trainY, devY, feature_functions):\n",
    "    trainX_feat=build_features(trainX, feature_functions)\n",
    "    devX_feat=build_features(devX, feature_functions)\n",
    "\n",
    "    # just create vocabulary from features in *training* data\n",
    "    feature_vocab=create_vocab(trainX_feat)\n",
    "\n",
    "    trainX_ids=features_to_ids(trainX_feat, feature_vocab)\n",
    "    devX_ids=features_to_ids(devX_feat, feature_vocab)\n",
    "    \n",
    "    logreg = linear_model.LogisticRegression(C=1.0, solver='lbfgs', penalty='l2', max_iter=10000)\n",
    "    logreg.fit(trainX_ids, trainY)\n",
    "    print(\"Accuracy: %.3f\" % logreg.score(devX_ids, devY)) \n",
    "    return logreg, feature_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_weights(clf, vocab, n=10):\n",
    "\n",
    "    reverse_vocab=[None]*len(clf.coef_[0])\n",
    "    for k in vocab:\n",
    "        reverse_vocab[vocab[k]]=k\n",
    "\n",
    "    if len(clf.classes_) == 2:\n",
    "        \n",
    "        weights=clf.coef_[0]\n",
    "        print(\"Features predicting negative reviews:\")\n",
    "        for feature, weight in sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))[:n]:\n",
    "            print(\"%.3f\\t%s\" % (weight, feature))\n",
    "\n",
    "        print()\n",
    "        print(\"Features predicting positive reviews:\")\n",
    "        for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
    "            print(\"%.3f\\t%s\" % (weight, feature))\n",
    "\n",
    "    else:  \n",
    "        for i, cat in enumerate(clf.classes_):\n",
    "\n",
    "            weights=clf.coef_[i]\n",
    "\n",
    "            for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
    "                print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\t0.500\n"
     ]
    }
   ],
   "source": [
    "baseline=majority_class(trainY,devY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function trains a model and returns the predicted and true labels for test data\n",
    "def evaluate(trainX, devX, trainY, devY, feature_functions):\n",
    "    trainX_feat=build_features(trainX, feature_functions)\n",
    "    devX_feat=build_features(devX, feature_functions)\n",
    "\n",
    "    # just create vocabulary from features in *training* data\n",
    "    feature_vocab=create_vocab(trainX_feat)\n",
    "\n",
    "    trainX_ids=features_to_ids(trainX_feat, feature_vocab)\n",
    "    devX_ids=features_to_ids(devX_feat, feature_vocab)\n",
    "    \n",
    "    logreg = linear_model.LogisticRegression(C=1.0, solver='lbfgs', penalty='l2', max_iter=10000)\n",
    "    logreg.fit(trainX_ids, trainY)\n",
    "    predictions=logreg.predict(devX_ids)\n",
    "    return (predictions, devY)\n",
    "\n",
    "def binomial_test(predictions, truth, baseline, significance_level=0.95):\n",
    "    correct=[]\n",
    "    for pred, gold in zip(predictions, truth):\n",
    "        correct.append(int(pred==gold))\n",
    "        \n",
    "    success_rate=np.mean(correct)\n",
    "\n",
    "    # two-tailed test\n",
    "    critical_value=(1-significance_level)/2\n",
    "    # ppf finds z such that p(X < z) = critical_value\n",
    "    z_alpha=-1*norm.ppf(critical_value)\n",
    "    print(\"Critical value: %.3f\\tz_alpha: %.3f\" % (critical_value, z_alpha))\n",
    "    \n",
    "    # the standard error is the square root of the variance/sample size\n",
    "    # the variance for a binomial test is p*(1-p)\n",
    "    standard_error=sqrt((success_rate*(1-success_rate))/len(correct))\n",
    "\n",
    "    Z=(success_rate-baseline)/standard_error\n",
    "    lower=success_rate-z_alpha*standard_error\n",
    "    upper=success_rate+z_alpha*standard_error\n",
    "    pval=norm.cdf(-abs(Z))\n",
    "    print (\"Accuracy: %.3f, n = %s\" % (success_rate, len(correct)))\n",
    "    print(\"%s%% Confidence interval: [%.3f,%.3f]\" % (significance_level*100, lower, upper))\n",
    "\n",
    "    print(\"Z score: %.3f\" % Z)\n",
    "    print(\"p-value: %.5f\" % pval)\n",
    "\n",
    "    print (\"Critical region corresponding to z_alpha=[%.3f,%.3f]: [%.3f, %.3f]\" % (-z_alpha, z_alpha, baseline-z_alpha*standard_error, baseline+z_alpha*standard_error))\n",
    "    print (\"Can we reject null that %.3f is different from %.3f at %s significance level? %s\" % (success_rate, baseline, significance_level*100, \"Yes\" if Z < -z_alpha or Z > z_alpha else \"No\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the impact of different feature functions by evaluating them below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.610\n"
     ]
    }
   ],
   "source": [
    "# run classification on sentiment_dict_feature\n",
    "features=[sentiment_dictionary_feature]\n",
    "clf, vocab=pipeline(trainX, devX, trainY, devY, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IF you want to print the coefficients for any of the models you train, you can do so like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print_weights(clf, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.887\n"
     ]
    }
   ],
   "source": [
    "# run classification on unigram_feature\n",
    "features=[unigram_feature]\n",
    "clf, vocab=pipeline(trainX, devX, trainY, devY, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get top features from unigram feature set: \n",
      "\n",
      "Features predicting negative reviews:\n",
      "-2.606\tUNIGRAM_4/10\n",
      "-2.478\tUNIGRAM_worst\n",
      "-2.158\tUNIGRAM_waste\n",
      "-1.847\tUNIGRAM_poorly\n",
      "-1.726\tUNIGRAM_awful\n",
      "-1.689\tUNIGRAM_3/10\n",
      "-1.684\tUNIGRAM_disappointment\n",
      "-1.536\tUNIGRAM_disappointing\n",
      "-1.420\tUNIGRAM_boring\n",
      "-1.351\tUNIGRAM_Avoid\n",
      "\n",
      "Features predicting positive reviews:\n",
      "2.685\tUNIGRAM_7/10\n",
      "1.583\tUNIGRAM_wonderfully\n",
      "1.510\tUNIGRAM_8/10\n",
      "1.459\tUNIGRAM_7\n",
      "1.453\tUNIGRAM_excellent\n",
      "1.433\tUNIGRAM_perfect\n",
      "1.421\tUNIGRAM_Excellent\n",
      "1.411\tUNIGRAM_refreshing\n",
      "1.257\tUNIGRAM_10/10\n",
      "1.251\tUNIGRAM_8\n"
     ]
    }
   ],
   "source": [
    "print(\"Get top features from unigram feature set: \\n\")\n",
    "print_weights(clf, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.882\n"
     ]
    }
   ],
   "source": [
    "# run classification on bigram_feature \n",
    "features=[new_feature_class_one]\n",
    "clf, vocab=pipeline(trainX, devX, trainY, devY, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get top features from bigram feature set: \n",
      "\n",
      "Features predicting negative reviews:\n",
      "-2.439\tBIGRAM_the worst\n",
      "-1.469\tBIGRAM_waste of\n",
      "-1.206\tBIGRAM_awful .\n",
      "-1.199\tBIGRAM_bad .\n",
      "-1.068\tBIGRAM_. 4/10\n",
      "-0.963\tBIGRAM_boring .\n",
      "-0.963\tBIGRAM_at all\n",
      "-0.930\tBIGRAM_terrible .\n",
      "-0.914\tBIGRAM_not worth\n",
      "-0.902\tBIGRAM_so bad\n",
      "\n",
      "Features predicting positive reviews:\n",
      "1.181\tBIGRAM_. Great\n",
      "1.117\tBIGRAM_the best\n",
      "1.108\tBIGRAM_my favorite\n",
      "0.924\tBIGRAM_is great\n",
      "0.914\tBIGRAM_excellent .\n",
      "0.903\tBIGRAM_love this\n",
      "0.901\tBIGRAM_an excellent\n",
      "0.888\tBIGRAM_a great\n",
      "0.885\tBIGRAM_is excellent\n",
      "0.876\tBIGRAM_on DVD\n"
     ]
    }
   ],
   "source": [
    "print(\"Get top features from bigram feature set: \\n\")\n",
    "print_weights(clf, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm a bit surprised that running classification on unigram and bigram features individually yields very similar classification accuracy. Moreover, it turns out that review scores do matter in predicting binary sentiment. For instance, a low score of 3/10 or 4/10 often appears in negative reviews, while higher scores like 7/10 and 8/10 are among determining features in predicting positive reviews. I believe this is something people often overlook when predicting reviews tied to a scoring mechanism. People's sentiment toward the movie is straightforwardly expressed through the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.598\n"
     ]
    }
   ],
   "source": [
    "# run classification on keyword_feature \n",
    "features=[new_feature_class_two]\n",
    "clf, vocab=pipeline(trainX, devX, trainY, devY, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get top features from all cap feature set: \n",
      "\n",
      "Features predicting negative reviews:\n",
      "-0.723\tallcaps_in_neg_dict\n",
      "-0.487\tallcaps_in_pos_dict\n",
      "\n",
      "Features predicting positive reviews:\n",
      "-0.487\tallcaps_in_pos_dict\n",
      "-0.723\tallcaps_in_neg_dict\n"
     ]
    }
   ],
   "source": [
    "print(\"Get top features from all cap feature set: \\n\")\n",
    "print_weights(clf, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the second feature set, which includes all-cap tokens, is not high. The coefficients are both negative, differing only in magnitude. This suggests that the classification model does not distinguish between the two binary label outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.883\n"
     ]
    }
   ],
   "source": [
    "features=[new_feature_class_one, new_feature_class_two]\n",
    "clf, vocab=pipeline(trainX, devX, trainY, devY, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.899\n"
     ]
    }
   ],
   "source": [
    "features=[unigram_feature, new_feature_class_one, new_feature_class_two]\n",
    "clf, vocab=pipeline(trainX, devX, trainY, devY, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features predicting negative reviews:\n",
      "-1.580\tUNIGRAM_worst\n",
      "-1.411\tUNIGRAM_awful\n",
      "-1.272\tUNIGRAM_boring\n",
      "-1.269\tUNIGRAM_waste\n",
      "-1.231\tBIGRAM_the worst\n",
      "-1.104\tUNIGRAM_poor\n",
      "-1.099\tUNIGRAM_terrible\n",
      "-1.085\tUNIGRAM_bad\n",
      "-1.054\tUNIGRAM_4/10\n",
      "-0.991\tUNIGRAM_poorly\n",
      "\n",
      "Features predicting positive reviews:\n",
      "1.250\tUNIGRAM_excellent\n",
      "1.094\tUNIGRAM_perfect\n",
      "1.000\tUNIGRAM_7/10\n",
      "0.900\tUNIGRAM_amazing\n",
      "0.887\tUNIGRAM_7\n",
      "0.822\tUNIGRAM_wonderful\n",
      "0.793\tUNIGRAM_great\n",
      "0.713\tBIGRAM_. Great\n",
      "0.712\tUNIGRAM_true\n",
      "0.712\tBIGRAM_better than\n"
     ]
    }
   ],
   "source": [
    "print_weights(clf, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the final model including all three feature sets yield the highest classification accuracy. There is a mix of unigram/bigram features, typically adjectives and review scores. It is interesting that adjectives in comparative (e.g. `better than`) and superlative forms (e.g. `the worst`) are among the list, which are defining keywords that express one's sentiment towards the movie. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
