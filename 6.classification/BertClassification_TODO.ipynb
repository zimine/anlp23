{"cells":[{"cell_type":"markdown","metadata":{"id":"sDh-kfcnxKPI"},"source":["Thie notebook explores using BERT for text classification.  Before starting, change the runtime to GPU: Runtime > Change runtime type > Hardware accelerator: GPU."]},{"cell_type":"markdown","metadata":{"id":"3m2nlYY_dnXP"},"source":["First, create a folder named ANLP23_data in your Google drive account, and copy this notebook to that folder, along with `data/convote`, `data/loc` and `data/lmrd` from the Github repo.  Double click on this notebook from your drive account, which will open it in the Google Colab environment.  Begin executing the cells from that environment.\n","\n","Let's give this notebook access to the data in your ANLP23 folder so we can train and evaluate BERT on the `convote` data.  (Note you are only providing this access to yourself as you execute this notebook.)  You can give Colab notebooks access to other data in the same way (by uploading it first to your Drive account, and then providing access here)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p4nx0eL5cuTd"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kRyQBPDKxczl"},"outputs":[],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mWZy26Ld0nGo"},"outputs":[],"source":["from transformers import BertModel, BertTokenizer\n","import torch\n","from tqdm import tqdm\n","import torch.nn as nn\n","import numpy as np\n","import random\n","import time"]},{"cell_type":"markdown","metadata":{"id":"g_N5SRVPyvDt"},"source":["Double-check that this notebook is running on the GPU (this should \"Running on cuda\")."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QTsJHWfVzS6Y"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Running on {}\".format(device))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iH5KcMBMxKPP"},"outputs":[],"source":["def read_labels(filename):\n","    labels={}\n","    with open(filename) as file:\n","        for line in file:\n","            cols = line.split(\"\\t\")\n","            label = cols[0]\n","            if label not in labels:\n","                labels[label]=len(labels)\n","    return labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZUOIgxLrxKPQ"},"outputs":[],"source":["def read_data(filename, labels, max_data_points=None):\n","    \"\"\"\n","    :param filename: the name of the file\n","    :return: list of tuple ([word index list], label)\n","    as input for the forward and backward function\n","    \"\"\"\n","    data = []\n","    data_labels = []\n","    with open(filename) as file:\n","        for line in file:\n","            cols = line.split(\"\\t\")\n","            label = cols[0]\n","            text = cols[1]\n","\n","            data.append(text)\n","            data_labels.append(labels[label])\n","\n","\n","    # shuffle the data\n","    tmp = list(zip(data, data_labels))\n","    random.shuffle(tmp)\n","    data, data_labels = zip(*tmp)\n","\n","    if max_data_points is None:\n","        return data, data_labels\n","\n","    return data[:max_data_points], data_labels[:max_data_points]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eSMOvzrNxKPS"},"outputs":[],"source":["labels=read_labels(\"/content/drive/MyDrive/ANLP23_data/convote/train.tsv\")\n","print(labels)"]},{"cell_type":"markdown","metadata":{"id":"nvQKP6t2xKPS"},"source":["We'll limit the training and dev data to 1,000 data points for this exercise."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JmHGHDJDxKPS"},"outputs":[],"source":["train_x, train_y=read_data(\"/content/drive/MyDrive/ANLP23_data/convote/train.tsv\", labels, max_data_points=1000)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lri3K6TsxKPT"},"outputs":[],"source":["dev_x, dev_y=read_data(\"/content/drive/MyDrive/ANLP23_data/convote/dev.tsv\", labels, max_data_points=1000)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0aMsvhvkxKPT"},"outputs":[],"source":["def evaluate(model, all_x, all_y):\n","    model.eval()\n","    corr = 0.\n","    total = 0.\n","    with torch.no_grad():\n","        idx=0\n","        for x, y in zip(all_x, all_y):\n","\n","            idx+=1\n","            y_preds=model.forward(x)\n","            for idx, y_pred in enumerate(y_preds):\n","                prediction=torch.argmax(y_pred)\n","                if prediction == y[idx]:\n","                    corr += 1.\n","                total+=1\n","    return corr/total"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uyE_hJzpxKPU"},"outputs":[],"source":["class BERTClassifier(nn.Module):\n","\n","    def __init__(self, params):\n","        super().__init__()\n","\n","        self.model_name=params[\"model_name\"]\n","        self.tokenizer = BertTokenizer.from_pretrained(self.model_name, do_lower_case=params[\"doLowerCase\"], do_basic_tokenize=False)\n","        self.bert = BertModel.from_pretrained(self.model_name)\n","\n","        self.num_labels = params[\"label_length\"]\n","\n","        self.fc = nn.Linear(params[\"embedding_size\"], self.num_labels)\n","\n","    def get_batches(self, all_x, all_y, batch_size=32, max_toks=256):\n","\n","        \"\"\" Get batches for input x, y data, with data tokenized according to the BERT tokenizer\n","      (and limited to a maximum number of WordPiece tokens \"\"\"\n","\n","        batches_x=[]\n","        batches_y=[]\n","\n","        for i in range(0, len(all_x), batch_size):\n","\n","            current_batch=[]\n","\n","            x=all_x[i:i+batch_size]\n","\n","            batch_x = self.tokenizer(x, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_toks)\n","            batch_y=all_y[i:i+batch_size]\n","\n","            batches_x.append(batch_x.to(device))\n","            batches_y.append(torch.LongTensor(batch_y).to(device))\n","\n","        return batches_x, batches_y\n","\n","\n","    def forward(self, batch_x):\n","\n","        bert_output = self.bert(input_ids=batch_x[\"input_ids\"],\n","                         attention_mask=batch_x[\"attention_mask\"],\n","                         token_type_ids=batch_x[\"token_type_ids\"],\n","                         output_hidden_states=True)\n","\n","        bert_hidden_states = bert_output['hidden_states']\n","\n","        # We're going to represent an entire document just by its [CLS] embedding (at position 0)\n","        out = bert_hidden_states[-1][:,0,:]\n","\n","        out = self.fc(out)\n","\n","        return out#.squeeze()"]},{"cell_type":"markdown","metadata":{"id":"EC3ysAoFfX4n"},"source":["Now let's train BERT on this data.  A few practicalities of this environment: if you encounter an out of memory error:\n","\n","* Reset the notebook (Runtime > Factory reset runtime) and execute all cells from the beginning.\n","* If your `max_length` is high, try reducing the `batch_size` in `get_batches` above.\n","\n","Even on a GPU, BERT can take a long time to train, so you might try experimenting first with smaller `max_data_points` above. before running it on the full training data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WZtoU7jzxKPU"},"outputs":[],"source":["def train_and_evaluate(bert_model_name, model_filename, train_x, train_y, dev_x, dev_y, labels, embedding_size=768, doLowerCase=None):\n","\n","  start_time=time.time()\n","  bert_model = BERTClassifier(params={\"doLowerCase\": doLowerCase, \"model_name\": bert_model_name, \"embedding_size\":embedding_size, \"label_length\": len(labels)})\n","  bert_model.to(device)\n","\n","  batch_x, batch_y = bert_model.get_batches(train_x, train_y)\n","  dev_batch_x, dev_batch_y = bert_model.get_batches(dev_x, dev_y)\n","\n","  optimizer = torch.optim.Adam(bert_model.parameters(), lr=1e-5)\n","  cross_entropy=nn.CrossEntropyLoss()\n","\n","  num_epochs=5\n","  best_dev_acc = 0.\n","\n","  for epoch in range(num_epochs):\n","      bert_model.train()\n","\n","      # Train\n","      for x, y in tqdm(list(zip(batch_x, batch_y))):\n","          y_pred = bert_model.forward(x)\n","          loss = cross_entropy(y_pred.view(-1, bert_model.num_labels), y.view(-1))\n","          optimizer.zero_grad()\n","          loss.backward()\n","          optimizer.step()\n","\n","      # Evaluate\n","      dev_accuracy=evaluate(bert_model, dev_batch_x, dev_batch_y)\n","      if epoch % 1 == 0:\n","          print(\"Epoch %s, dev accuracy: %.3f\" % (epoch, dev_accuracy))\n","          if dev_accuracy > best_dev_acc:\n","              torch.save(bert_model.state_dict(), model_filename)\n","              best_dev_acc = dev_accuracy\n","\n","  bert_model.load_state_dict(torch.load(model_filename))\n","  print(\"\\nBest Performing Model achieves dev accuracy of : %.3f\" % (best_dev_acc))\n","  print(\"Time: %.3f seconds ---\" % (time.time() - start_time))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CgLvw1VRiqR_"},"outputs":[],"source":["train_and_evaluate(\"bert-base-cased\", \"convote-bert-base-cased\", train_x, train_y, dev_x, dev_y, labels, embedding_size=768, doLowerCase=False)"]},{"cell_type":"markdown","metadata":{"id":"SpXSzKa8ghA4"},"source":["**Q1**.  Train BERT on your classification data (from the FeatureExploration_TODO.ipynb exercise) by following the steps outlined above.  At this point you have evaluated a number of different classification methods for that data.  Report those development accuracies below.  Be sure to enable a fair comparison by training and evaluating each method on **the same amount of data**.  Provide a one-sentence short description that details the essentials of each method (e.g., major feature classes, etc.), and tell us what data you're running these models on (either one of the three choices above, or your own data).\n","\n","|Method|Short description|Accuracy|\n","|--|--|--|\n","|Majority class||\n","|Logistic regression (6.classification/FeatureExploration.ipynb)||\n","|BERT||\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZB7gPBAMlWeX"},"source":["**Q2.** As you can see, training `bert-base` can be expensive.  Google has released a number of [smaller BERT models](https://github.com/google-research/bert) with fewer layers (2, 4, 6, 8, 10) and smaller dimensions (128, 256, 512) that effectively trade off accuracy for speed.  Select three of these models and train them; report both their accuracy and training time.\n","\n","\n","\n","To use these models in the huggingface library that we have been using, the huggingface name of the model can be derived from the URL linking to it:\n","\n","https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-2_H-128_A-2.zip -> `google/bert_uncased_L-2_H-128_A-2`\n","\n","All of these smaller models are uncased (so all text is lowercase), so be sure to set `doLowerCase` to be true.  You'll also need to change the `embedding_size` parameter to this function based on the H value from the model (listed both on the BERT Github page and in the model's URL).  One sample model is provided below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c1xQgyJs56En"},"outputs":[],"source":["train_and_evaluate(\"google/bert_uncased_L-2_H-128_A-2\", \"lmrd-uncased_L-2_H-128_A-2\", train_x, train_y, dev_x, dev_y, labels, embedding_size=128, doLowerCase=True)"]},{"cell_type":"markdown","source":["To turn in:\n","\n","- Go to `File > Download > Download .ipynb` and save your notebook.\n","- In your browser, print this page to save as PDF.\n","- Upload both your .ipynb and .pdf files to bCourses as usual."],"metadata":{"id":"KKAfd-ARQLt2"}}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"nbformat":4,"nbformat_minor":0}