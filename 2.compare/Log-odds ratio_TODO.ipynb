{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-odds ratio with an informative (and uninformative) Dirichlet prior (described in [Monroe et al. 2009, Fighting Words](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf)) is a common method for finding distinctive terms in two datasets (see [Jurafsky et al. 2014](https://firstmonday.org/ojs/index.php/fm/article/view/4944/3863) for an example article that uses it to make an empirical argument). This method for finding distinguishing words combines a number of desirable properties:\n",
    "\n",
    "* it specifies an intuitive metric (the log-odds) for the ratio of two probabilities\n",
    "* it can incorporate prior information in the form of pseudocounts, which can either act as a smoothing factor (in the uninformative case) or incorporate real information about the expected frequency of words overall.\n",
    "* it accounts for variability of a frequency estimate by essentially converting the log-odds to a z-score.\n",
    "\n",
    "In this homework you will implement this ratio for a dataset of your choice to characterize the words that differentiate each one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first job is to find two datasets with some interesting opposition -- e.g., news articles from CNN vs. FoxNews, books written by Charles Dickens vs. James Joyce, screenplays of dramas vs. comedies.  Be creative -- this should be driven by what interests you and should reflect your own originality. **This dataset cannot come from Kaggle**.  Feel feel to use web scraping (see [here](https://github.com/CU-ITSS/Web-Data-Scraping-S2023) for a great tutorial) or manually copying/pasting text.  Aim for more than 10,000 tokens for each dataset. \n",
    "   \n",
    "Save those datasets in two files: \"class1_dataset.txt\" and \"class2_dataset.txt\" \n",
    "\n",
    "Q1. Describe each of those datasets and their source in 100-200 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm interested in word choices in different movie genres so I selected two representative movies of the Sci-Fi (2001: A Space Odyssey) and Romance (Jerry Maguire) genres, respectively. The scripts were sourced from [SimpleScripts](https://www.simplyscripts.com/), a database of open-source screenplays. Both movies had comparable length in terms of screen time. Upon closer inspection, the scripts also share a similar number of tokens (~ 25,000). It is expected that words such as `space`, `machine`, `time` will be most relevant to the Sci-Fi genre, and for the Romance genre, more likley is the case we see words related to emotions, feelings, or mentions of main characters, and descriptions of the scenarios. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Tokenize those texts by filling out the `read_and_tokenize` function below (your choice of tokenizer). The input is a filename and the output should be a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_and_tokenize(filename):\n",
    "    \n",
    "    # read in the script and lowercase all tokens\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        text = file.read().lower()  \n",
    "    \n",
    "    # sentence tokenization\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # tokenization\n",
    "    words = []\n",
    "    for sent in sents:\n",
    "        words.append(nltk.word_tokenize(sent))\n",
    "    \n",
    "    # flatten the words list to return a list of tokens\n",
    "    # also remove punctuations identified in string.punctuation\n",
    "    alltokens = [word for sent in words for word in sent if word not in string.punctuation]\n",
    "    \n",
    "    # if not remove punctuation, use the code below\n",
    "    # alltokens = [word for sent in words for word in sent]\n",
    "    \n",
    "    return alltokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# change these file paths to wherever the datasets you created above live.\n",
    "class1_tokens=read_and_tokenize(\"../data/scripts/scifiscripts.com_scripts_2001.txt\")\n",
    "class2_tokens=read_and_tokenize(\"../data/scripts/awesomefilm.com_script_jerryMaguire.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tokens in class 1 is: 25195\n",
      "The number of tokens in class 2 is: 28505\n"
     ]
    }
   ],
   "source": [
    "# check token size for each class\n",
    "print(\"The number of tokens in class 1 is: %.f\" % len(class1_tokens))\n",
    "print(\"The number of tokens in class 2 is: %.f\" % len(class2_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.  Now let's find the words that characterize each of those sources (with respect to the other). Implement the log-odds ratio with an uninformative Dirichlet prior.  This value, $\\hat\\zeta_w^{(i-j)}$ for word $w$ reflecting the difference in usage between corpus $i$ and corpus $j$, is given by the following equation:\n",
    "\n",
    "$$\n",
    "\\hat\\zeta_w^{(i-j)}= {\\hat{d}_w^{(i-j)} \\over \\sqrt{\\sigma^2\\left(\\hat{d}_w^{(i-j)}\\right)}}\n",
    "$$\n",
    "\n",
    "Where: \n",
    "\n",
    "$$\n",
    "\\hat{d}_w^{(i-j)} = \\log \\left({y_w^i + \\alpha_w} \\over {n^i + \\alpha_0 - y_w^i - \\alpha_w}) \\right) -  \\log \\left({y_w^j + \\alpha_w} \\over {n^j + \\alpha_0 - y_w^j - \\alpha_w}) \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma^2\\left(\\hat{d}_w^{(i-j)}\\right) \\approx {1 \\over {y_w^i + \\alpha_w}} + {1 \\over {y_w^j + \\alpha_w} }\n",
    "$$\n",
    "\n",
    "And:\n",
    "\n",
    "* $y_w^i = $ count of word $w$ in corpus $i$ (likewise for $j$)\n",
    "* $\\alpha_w$ = 0.01\n",
    "* $V$ = size of vocabulary (number of distinct word types)\n",
    "* $\\alpha_0 = V * \\alpha_w$\n",
    "* $n^i = $ number of words in corpus $i$ (likewise for $j$)\n",
    "\n",
    "In this example, the two corpora are your class1 dataset (e.g., $i$ = your class1) and your class2 dataset (e.g., $j$ = class2). Using this metric, print out the 25 words most strongly aligned with class1, and 25 words most strongly aligned with class2.  Again, consult [Monroe et al. 2009, Fighting Words](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf) for more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def logodds_with_uninformative_prior(one_tokens, two_tokens, display=25):\n",
    "    \n",
    "    # get unique tokens for each class\n",
    "    class1_v = set(one_tokens)\n",
    "    class2_v = set(two_tokens)\n",
    "    \n",
    "    # get the vocab list - distinct word types from two classes\n",
    "    vocab = list(class1_v | class2_v)\n",
    "    \n",
    "    # get word count for each token in each class\n",
    "    # token not in the class gets 0 count\n",
    "    class1_cnt = {word:one_tokens.count(word) if word in class1_v else 0 for word in vocab}\n",
    "    class2_cnt = {word:two_tokens.count(word) if word in class2_v else 0 for word in vocab}\n",
    "    \n",
    "    assert len(class1_cnt) == len(class2_cnt) == len(vocab)\n",
    "    \n",
    "    # get vocab size of each class\n",
    "    class1_num = len(one_tokens)\n",
    "    class2_num = len(two_tokens)\n",
    "    \n",
    "    # alpha setting\n",
    "    alpha_w = 0.01\n",
    "    alpha_0 = len(vocab) * alpha_w \n",
    "    \n",
    "    # initialize a dict to store log odds ratios\n",
    "    lor = {}\n",
    "    \n",
    "    # loop over to get lor\n",
    "    for word in vocab:\n",
    "        I = (class1_cnt[word] + alpha_w) / (class1_num + alpha_0 - class1_cnt[word] - alpha_w)\n",
    "        J = (class2_cnt[word] + alpha_w) / (class2_num + alpha_0 - class2_cnt[word] - alpha_w)\n",
    "        d = (1 / (class1_cnt[word] + alpha_w)) + (1 / (class2_cnt[word] + alpha_w))\n",
    "        ratio = (np.log(I) - np.log(J)) / np.sqrt(d)\n",
    "        lor[word] = ratio\n",
    "    \n",
    "    # sort the lor dict by values \n",
    "    sorted_lor = dict(sorted(lor.items(), key=lambda item: item[1], reverse=True))\n",
    "    scores = list(sorted_lor.items())\n",
    "    \n",
    "    # return top and last 25 tokens\n",
    "    return scores[:display], scores[-display:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words that most aligned with the Sci-Fi movie--2001: A Space Odyssey:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('--', 68.82342605357024),\n",
       " ('had', 5.586309810975934),\n",
       " ('space', 4.626690273113128),\n",
       " ('earth', 4.45198097545501),\n",
       " ('its', 4.310250655343011),\n",
       " ('any', 4.238909992584407),\n",
       " ('control', 4.221181072603766),\n",
       " ('would', 3.6724336732334875),\n",
       " ('been', 3.604349770236324),\n",
       " ('well', 3.588526684311813),\n",
       " ('doors', 3.4575805734136678),\n",
       " ('others', 3.2732512611545554),\n",
       " ('yes', 3.1842696557593575),\n",
       " ('slowly', 3.1705022245375485),\n",
       " ('command', 3.1150770706352646),\n",
       " ('mission', 3.111298161286163),\n",
       " ('area', 3.0716941110192413),\n",
       " ('their', 3.03416884626844),\n",
       " ('michaels', 2.841531970932135),\n",
       " ('true', 2.8258075947089156),\n",
       " ('please', 2.8258075947089156),\n",
       " ('hope', 2.7005519067672092),\n",
       " ('tv', 2.7005519067672092),\n",
       " ('child', 2.672553360644189),\n",
       " ('inside', 2.6716649065882927)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Words that most aligned with the Sci-Fi movie--2001: A Space Odyssey:\\n\")\n",
    "logodds_with_uninformative_prior(class1_tokens, class2_tokens)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list above gives the top 25 words that most characterize the writing style of the Sci-Fi movie *2001: A Space Odyssey*. There are a number of intuitive examples: `space`, `earth`, `control`, `command`, `mission`, `area`, and character names such as `michaels`. There are also less intuitive ones, for instance, the most distinctive token `--`, which is used as a line separator in the original script to separate scenes. This token, among others, could be further removed during preprocessing to improve accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words that most aligned with the Romance movie--Jerry Maguire:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(\"n't\", -5.428547395403892),\n",
       " ('in', -5.542443736374114),\n",
       " ('is', -5.694981912898374),\n",
       " ('day', -5.750810208778898),\n",
       " ('his', -5.809561882296956),\n",
       " ('do', -5.922333833699507),\n",
       " ('room', -5.924146925486849),\n",
       " (\"'m\", -5.942145767086766),\n",
       " ('int', -6.322948129128177),\n",
       " ('night', -6.47454817747116),\n",
       " ('with', -6.677140054541707),\n",
       " ('``', -6.758807274835594),\n",
       " ('her', -7.159767774987964),\n",
       " ('my', -7.165257175308368),\n",
       " ('me', -7.496562226880489),\n",
       " ('him', -7.608793271750172),\n",
       " ('on', -7.662279649651411),\n",
       " (\"''\", -8.148035251120648),\n",
       " ('she', -8.248173887980224),\n",
       " ('...', -8.931941782113322),\n",
       " ('he', -9.376475351759723),\n",
       " ('a', -10.067238918738651),\n",
       " ('you', -10.12467992561834),\n",
       " ('i', -10.75946397829889),\n",
       " (\"'s\", -11.549952273385347)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Words that most aligned with the Romance movie--Jerry Maguire:\\n\")\n",
    "logodds_with_uninformative_prior(class1_tokens, class2_tokens)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last 25 tokens from the sorted dictionary gives the tokens that are best aligned with the Romance movie *Jerry Maguire*. It is interesting that most of these tokens are pronouns, `his`, `her`, `he`, `she`, `me`, and `you`. Pronouns occur most of the times to depict the actions and thoughts of characters, from the third-person point of view, commonly seen in screenplay. There are also words describe the time and location of the scenes--`day`, `night`, `room`, `with`, also relevant to the genre.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
