{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook examines the words that distinguish the [2020 Democrat party platform](https://democrats.org/wp-content/uploads/2020/08/2020-Democratic-Party-Platform.pdf) from the [2020/2016 Republican party platform](https://prod-cdn-static.gop.com/docs/Resolution_Platform_2020.pdf) (both OCR'd with Abbyy FineReader), using the Chi-Square and the Mann-Whitney test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import nltk\n",
    "import math\n",
    "import operator\n",
    "from collections import Counter\n",
    "from scipy.stats import mannwhitneyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read(filename):\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        # lowercase text\n",
    "        return file.read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "democratText=read(\"../data/democrat_platform_2020.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "republicanText=read(\"../data/republican_platform_2020.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore your assumptions between the words you think will most distinguish the Democrat and Republican platforms.  Before looking at the results of the tests, what words do you think will be comparatively distinct to both?  (If you're not familiar with either, scan the platforms linked above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypotheses: \n",
    "\n",
    "Democrates are likely to be characterized by key words such as freedom, liberty, women, right, but republicans are likely to feature key words such as justice, people, budget. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    return nltk.word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_counts(tokens):\n",
    "    counts=Counter()\n",
    "    for token in tokens:\n",
    "        counts[token]+=1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\chi^2$ test as used in the comparison of different texts is designed to measure how statistically significant the distriubtion of counts in a 2x2 contingency table is.  Use the following function to analyze the difference between the platforms.  How do the most distinct terms comport with your assumptions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chi_square(one_counts, two_counts):\n",
    "\n",
    "    one_sum=0.\n",
    "    two_sum=0.\n",
    "    vocab={}\n",
    "    for word in one_counts:\n",
    "        one_sum+=one_counts[word]\n",
    "        vocab[word]=1\n",
    "    for word in two_counts:\n",
    "        vocab[word]=1\n",
    "        two_sum+=two_counts[word]\n",
    "\n",
    "    N=one_sum+two_sum\n",
    "    vals={}\n",
    "    \n",
    "    for word in vocab:\n",
    "        O11=one_counts[word]\n",
    "        O12=two_counts[word]\n",
    "        O21=one_sum-one_counts[word]\n",
    "        O22=two_sum-two_counts[word]\n",
    "        \n",
    "        # We'll use the simpler form given in Manning and Schuetze (1999) \n",
    "        # for 2x2 contingency tables: \n",
    "        # https://nlp.stanford.edu/fsnlp/promo/colloc.pdf, equation 5.7\n",
    "        \n",
    "        vals[word]=(N*(O11*O22 - O12*O21)**2)/((O11 + O12)*(O11+O21)*(O12+O22)*(O21+O22))\n",
    "        \n",
    "    sorted_chi = sorted(vals.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    one=[]\n",
    "    two=[]\n",
    "    for k,v in sorted_chi:\n",
    "        if one_counts[k]/one_sum > two_counts[k]/two_sum:\n",
    "            one.append(k)\n",
    "        else:\n",
    "            two.append(k)\n",
    "    \n",
    "    print (\"Democrat:\\n\")\n",
    "    for k in one[:20]:\n",
    "        print(\"%s\\t%s\" % (k,vals[k]))\n",
    "\n",
    "    print (\"\\n\\nRepublican:\\n\")\n",
    "    for k in two[:20]:\n",
    "        print(\"%s\\t%s\" % (k,vals[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "democrat_tokens=tokenize(democratText)\n",
    "democrat_counts=get_counts(democrat_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "republican_tokens=tokenize(republicanText)\n",
    "republican_counts=get_counts(republican_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Democrat:\n",
      "\n",
      "democrats\t363.5351093621297\n",
      "will\t359.5218076288936\n",
      "and\t216.96130188397586\n",
      "health\t136.8219687920415\n",
      "trump\t103.78980852228422\n",
      "including\t98.64837668848244\n",
      "care\t72.23926563658084\n",
      "workers\t69.87282904669634\n",
      "pandemic\t68.87152165109498\n",
      "believe\t59.449120769409085\n",
      "covid-19\t49.67983456368194\n",
      "color\t47.93560592228924\n",
      "ensure\t40.59885319041926\n",
      ",\t40.28339268660182\n",
      "native\t39.006436354265425\n",
      "housing\t38.63232476674289\n",
      "investments\t38.22087286926605\n",
      "affordable\t36.83400407434361\n",
      "expand\t34.39374416675337\n",
      "clean\t33.908189816758096\n",
      "\n",
      "\n",
      "Republican:\n",
      "\n",
      "the\t140.22224554283582\n",
      "of\t105.2173628369195\n",
      "—\t95.72655635923344\n",
      "government\t69.32917787095215\n",
      "republican\t64.14154427012217\n",
      "current\t63.01371009014764\n",
      "is\t57.36040628425552\n",
      "a\t56.96910698056252\n",
      "it\t55.4479146995238\n",
      "congress\t45.29329534988732\n",
      "their\t44.404098458032365\n",
      "be\t43.98452976297621\n",
      "urge\t42.89292820199374\n",
      "★\t42.49664909272419\n",
      "healthcare\t40.19863033601007\n",
      "its\t39.91111642081513\n",
      ":\t39.34546458484308\n",
      "call\t38.92239613060992\n",
      "republicans\t36.61108000787439\n",
      "must\t35.36099672285782\n"
     ]
    }
   ],
   "source": [
    "chi_square(democrat_counts, republican_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are these results surprising? Examine specific words to check their frequency in both datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words that are solely frequent in one corpus gets emphasized more, i.e. `democrates`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totals: R: 41484, D: 47627\n"
     ]
    }
   ],
   "source": [
    "print(\"Totals: R: %s, D: %s\" % (len(republican_tokens), len(democrat_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the -- R: 2369, D: 1910\n"
     ]
    }
   ],
   "source": [
    "word=\"the\"\n",
    "print(\"%s -- R: %s, D: %s\" % (word, republican_counts[word], democrat_counts[word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw earlier that $\\chi^2$ is not a perfect estimator since doesn't account for the \"burstiness\" of language -- if we see the word \"Dracula\" in a text, we're probably going to see it again in that same text. The occurrence of words are not independent random events; they are tightly coupled with each other. If we're trying to understanding the robust differences between two corpora, we might prefer to prioritize words that show up more frequently everywhere in corpus A (but not in corpus B) over those that show up only very frequently within narrow slice of A (such as one text in a genre, one chapter in a book, or one speaker when measuring the differences between policital parties). Use the following function to execute the Mann-Whitney test to account for this phenomenon while finding distinctive terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_differences(one_tokens, two_tokens):\n",
    "    one_N=len(one_tokens)\n",
    "    two_N=len(two_tokens)\n",
    "    \n",
    "    one_counts=Counter()\n",
    "    two_counts=Counter()\n",
    "    \n",
    "    vocab={}\n",
    "    for token in one_tokens:\n",
    "        one_counts[token]+=1\n",
    "        vocab[token]=1\n",
    "        \n",
    "    for token in two_tokens:\n",
    "        two_counts[token]+=1    \n",
    "        vocab[token]=1\n",
    "        \n",
    "    differences={}\n",
    "    for word in vocab:\n",
    "        freq1=one_counts[word]/one_N\n",
    "        freq2=two_counts[word]/two_N\n",
    "        \n",
    "        diff=freq1-freq2\n",
    "        differences[word]=diff\n",
    "        \n",
    "    return differences\n",
    "\n",
    "# convert a sequence of tokens into counts for each chunkLength-word window\n",
    "def get_chunk_counts(tokens, chunkLength):\n",
    "    chunks=[]\n",
    "    for i in range(0, len(tokens), chunkLength):\n",
    "            counts=Counter()\n",
    "            for j in range(chunkLength):\n",
    "                if i+j < len(tokens):\n",
    "                    counts[tokens[i+j]]+=1\n",
    "            chunks.append(counts)\n",
    "    return chunks\n",
    "\n",
    "# calculate mann-whitney test for each word in vocabulary\n",
    "def mann_whitney(one_tokens, two_tokens):\n",
    "\n",
    "    chunkLength=500\n",
    "    one_chunks=get_chunk_counts(one_tokens, chunkLength)\n",
    "    two_chunks=get_chunk_counts(two_tokens, chunkLength)\n",
    "    \n",
    "    # vocab is the union of terms in both sets\n",
    "    vocab={}\n",
    "    \n",
    "    for chunk in one_chunks:\n",
    "        for word in chunk:\n",
    "            vocab[word]=1\n",
    "    for chunk in two_chunks:\n",
    "        for word in chunk:\n",
    "            vocab[word]=1\n",
    "    \n",
    "    pvals={}\n",
    "    \n",
    "    for word in vocab:\n",
    "        \n",
    "        a=[]\n",
    "        b=[]\n",
    "        \n",
    "        # Note a and b can be different lengths (i.e., different sample sizes)\n",
    "        # \n",
    "        # See Mann and Whitney (1947), \"On a Test of Whether one of Two Random \n",
    "        # Variables is Stochastically Larger than the Other\"\n",
    "        # https://projecteuclid.org/download/pdf_1/euclid.aoms/1177730491\n",
    "        \n",
    "        # (This is part of their innovation over the case of equal sample sizes in Wilcoxon 1945)\n",
    "        \n",
    "        for chunk in one_chunks:\n",
    "            a.append(chunk[word])\n",
    "        for chunk in two_chunks:\n",
    "            b.append(chunk[word])\n",
    "\n",
    "        statistic,pval=mannwhitneyu(a,b, alternative=\"two-sided\")\n",
    "        \n",
    "        # We'll use the p-value as our quantity of interest.  [Note in the normal appproximation\n",
    "        # that Mann-Whitney uses to assess significance for large sample sizes, the significance \n",
    "        # of the raw statistic depends on the number of ties in the data, so the statistic itself\n",
    "        # isn't exactly comparable across different words]\n",
    "        pvals[word]=pval\n",
    "\n",
    "    return pvals\n",
    "    \n",
    "# calculate mann-whitneyfor each word in vocabulary and present the top 10 terms for each group\n",
    "def mann_whitney_analysis(one_tokens, two_tokens):\n",
    "    \n",
    "    pvals=mann_whitney(one_tokens, two_tokens)\n",
    "    \n",
    "    # Mann-Whitney tells us the significance of a term's difference in two groups, but we also \n",
    "    # need the directionality of that difference (whether it's used more by group A or group B. \n",
    "    \n",
    "    # Let's use our difference-in-proportions function above to check the directionality.  \n",
    "    # [Note we could also measure directionality by checking whether the Mann-Whitney statistic\n",
    "    # is greater or less than the mean=len(one_chunks)*len(two_chunks)*0.5.]\n",
    "\n",
    "    differences=count_differences(one_tokens, two_tokens)\n",
    "    \n",
    "    one_terms={k : pvals[k] for k in pvals if differences[k] <= 0}\n",
    "    two_terms={k : pvals[k] for k in pvals if differences[k] > 0}\n",
    "    \n",
    "    sorted_pvals = sorted(one_terms.items(), key=operator.itemgetter(1))\n",
    "    print(\"More Republican:\\n\")\n",
    "    for k,v in sorted_pvals[:10]:\n",
    "        print(\"%s\\t%.15f\" % (k,v))\n",
    "\n",
    "    print(\"\\nMore Democrat:\\n\")\n",
    "    sorted_pvals = sorted(two_terms.items(), key=operator.itemgetter(1))\n",
    "    for k,v in sorted_pvals[:10]:\n",
    "        print(\"%s\\t%.15f\" % (k,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More Republican:\n",
      "\n",
      "the\t0.000000000000028\n",
      "—\t0.000000000000037\n",
      "of\t0.000000000002035\n",
      "republican\t0.000000000003047\n",
      ".\t0.000000000006887\n",
      "current\t0.000000000007479\n",
      "a\t0.000000000138855\n",
      ":\t0.000000002872820\n",
      "republicans\t0.000000003159717\n",
      "urge\t0.000000008079836\n",
      "\n",
      "More Democrat:\n",
      "\n",
      "democrats\t0.000000000000000\n",
      "and\t0.000000000000000\n",
      "will\t0.000000000000000\n",
      "trump\t0.000000000000000\n",
      "including\t0.000000000001298\n",
      "believe\t0.000000000002003\n",
      "pandemic\t0.000000000082672\n",
      "covid-19\t0.000000000295251\n",
      "ensure\t0.000000015554296\n",
      "color\t0.000000056605160\n"
     ]
    }
   ],
   "source": [
    "mann_whitney_analysis(democrat_tokens, republican_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are the differences identified by Mann-Whitney similar to, and different from, those identified by $\\chi^2$?  What conclusions would you draw from the differences between these platforms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from the Mann-Whitney test are very similar to the words identified by chi-squared test. It might be the case that these salient words occur prevalently in each chunk. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
