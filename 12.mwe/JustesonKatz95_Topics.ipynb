{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores identifying multiword expressions using the part-of-speech filtering technique of Justeson and Katz (1995), \"[Technical terminology: some linguistic properties and an algorithm for identification in text](https://brenocon.com/JustesonKatz1995.pdf)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy, re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['ner,parser'])\n",
    "nlp.remove_pipe('ner')\n",
    "nlp.remove_pipe('parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTokens(filename, top=1000):\n",
    "    \n",
    "    \"\"\" Read the first *top* lines of an input file \"\"\"\n",
    "    docs=[]\n",
    "    with open(filename) as file:\n",
    "        for idx,line in enumerate(file):\n",
    "            docs.append(nlp(line))\n",
    "            if idx > top:\n",
    "                break\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=getTokens(\"../data/wiki.10K.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simplify the POS tags to make the regex easier to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tokens_to_simple_pos(tokens):\n",
    "    adjectives=set([\"JJ\", \"JJR\", \"JJS\"])\n",
    "    nouns=set([\"NN\", \"NNS\", \"NNP\", \"NNPS\"])\n",
    "\n",
    "    tags=[]\n",
    "    for x in tokens:\n",
    "        if x.tag_ in adjectives:\n",
    "            tags.append(\"ADJ\")\n",
    "        elif x.tag_ in nouns:\n",
    "            tags.append(\"NOUN\")\n",
    "        elif x.tag == \"IN\":\n",
    "            tags.append(\"PREP\")\n",
    "        else:\n",
    "            tags.append(\"O\")\n",
    "\n",
    "    tags=' '.join(tags)    \n",
    "    \n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getChar2TokenMap(tags):\n",
    "    \n",
    "    \"\"\"  We'll search over the postag sequence, so we need to get the token ID for any\n",
    "    character to be able to match the word token. \"\"\"\n",
    "    \n",
    "    ws=re.compile(\" \")\n",
    "    char2token={}\n",
    "\n",
    "    lastStart=0\n",
    "    for idx, m in enumerate(ws.finditer(tags)):\n",
    "        char2token[lastStart]=idx\n",
    "        lastStart=m.start()+1\n",
    "        \n",
    "    return char2token\n",
    "\n",
    "def getToken(tokenId, char2token):\n",
    "    \n",
    "    \"\"\" Find the token ID for given character in the POS sequence \"\"\"\n",
    "    while(tokenId > 0):\n",
    "        if tokenId in char2token:\n",
    "            return char2token[tokenId]\n",
    "        tokenId-=1\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find all sequences of POS tags that match the Justeson and Katz pattern of `(((ADJ|NOUN) )+|((ADJ|NOUN) )*(NOUN PREP )((ADJ|NOUN) )*)NOUN`\n",
    "\n",
    "\"In words, a candidate term is a multi-word noun phrase; and it either is a string of nouns and/or adjectives, ending in a noun, or it consists of two such strings, separated by a single preposition.\" (JK 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mwes_from_docs(docs, top_mwe=1000):\n",
    "    p = re.compile(\"(((ADJ|NOUN) )+|((ADJ|NOUN) )*(NOUN PREP )((ADJ|NOUN) )*)NOUN\")\n",
    "\n",
    "    mweCount=Counter()\n",
    "\n",
    "    for tokens in docs:\n",
    "        tags=convert_tokens_to_simple_pos(tokens)\n",
    "        char2token=getChar2TokenMap(tags)\n",
    "        words=[x.text for x in tokens]\n",
    "        \n",
    "        for m in p.finditer(tags):\n",
    "            startToken=getToken(m.start(),char2token)\n",
    "            endToken=getToken(m.end(),char2token)\n",
    "            mwe=' '.join(words[startToken:endToken+1])\n",
    "            mweCount[mwe]+=1\n",
    "\n",
    "    for k,v in mweCount.most_common(10):\n",
    "        print(k,v)\n",
    "        \n",
    "    # We'll define our MWE dictionary to be the *top_mwe* most frequent sequences matched.\n",
    "    \n",
    "    my_mwe=[k for (k,v) in mweCount.most_common(top_mwe)]\n",
    "    return my_mwe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mwe=get_mwes_from_docs(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's transform each MWE into a single token (e.g., replace `New York City` with `New_York_City`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceMWE(text, mweList):\n",
    "    \n",
    "    \"\"\" Replace all instances of MWEs in text with single token \n",
    "    \n",
    "    MWEs are ranked from longest to shortest so that longest replacements are made first (e.g.,\n",
    "    \"New York City\" is matched first before \"New York\")\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    sorted_by_length = sorted(mweList, key=len, reverse=True)\n",
    "    for mwe in sorted_by_length:\n",
    "        text=re.sub(re.escape(mwe), re.sub(\" \", \"_\", mwe), text)\n",
    "    return text.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedText=replaceMWE(\"The New York Times is located in New York City\", my_mwe)\n",
    "print(processedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_mwe_docs(docs, my_mwe):\n",
    "    mwe_docs=[]\n",
    "    for doc in docs:\n",
    "        processedText=replaceMWE(' '.join([x.text for x in doc]), my_mwe)    \n",
    "        mwe_docs.append(processedText)\n",
    "    return mwe_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. When we used topic modeling in `4.topics/TopicModel.ipynb`, we represented a document as a bag of words.  MWE allow us to add a little more structure (phrases) but still preserve that same basic accumption (a document is a bag of phrases and words).  Combine both of these methods to create a topic model that reasons over words and phrases.  The only thing you should have to change is how the input text is tokenized (i.e., using the `replaceMWE` function above).  Run that topic model on `data/plot_summaries.txt` (the same data used in the original `4.topics/TopicModel.ipynb` notebook). How would you characterize the difference between the topics that each model learns?\n",
    "\n",
    "(You'll find the code from `4.topics/TopicModel.ipynb` below to get you started.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import operator\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stopwords(filename):\n",
    "    stopwords={}\n",
    "    with open(filename) as file:\n",
    "        for line in file:\n",
    "            stopwords[line.rstrip()]=1\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = {k:1 for k in stopwords.words('english')}\n",
    "stop_words.update(read_stopwords(\"../data/jockers.stopwords\"))\n",
    "stop_words[\"'s\"]=1\n",
    "stop_words=list(stop_words.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(word, stopwords):\n",
    "    \n",
    "    \"\"\" Function to exclude words from a text \"\"\"\n",
    "\n",
    "    word=word.lower()\n",
    "    \n",
    "    # no stopwords\n",
    "    if word in stopwords:\n",
    "        return False\n",
    "    \n",
    "    # has to contain at least one letter\n",
    "    if re.search(\"[A-Za-z]\", word) is not None:\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docs(plotFile, metadataFile, stopwords):\n",
    "    \n",
    "    names={}\n",
    "    box={}\n",
    "    \n",
    "    with open(metadataFile, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            idd=cols[0]\n",
    "            name=cols[2]\n",
    "            boxoffice=cols[4]\n",
    "            if len(boxoffice) != 0:\n",
    "                box[idd]=int(boxoffice)\n",
    "                names[idd]=name\n",
    "    \n",
    "    n=5000\n",
    "    target_movies={}\n",
    "\n",
    "\n",
    "    sorted_box = sorted(box.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    for k, v in sorted_box[:n]:\n",
    "        target_movies[k]=names[k]\n",
    "    \n",
    "    docs=[]\n",
    "    names=[]\n",
    "   \n",
    "    with open(plotFile, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            idd=cols[0]\n",
    "            text=cols[1]\n",
    "            \n",
    "            if idd in target_movies:\n",
    "                tokens=nltk.word_tokenize(text.lower())\n",
    "                tokens=[x for x in tokens if filter(x, stopwords)]\n",
    "                docs.append(tokens)\n",
    "                name=target_movies[idd]\n",
    "                names.append(name)\n",
    "    return docs, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadataFile=\"../data/movie.metadata.tsv\"\n",
    "plotFile=\"../data/plot_summaries.txt\"\n",
    "data, doc_names=read_docs(plotFile, metadataFile, stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocab from data; restrict vocab to only the top 10K terms that show up in at least 5 documents \n",
    "# and no more than 50% of all documents\n",
    "\n",
    "dictionary = corpora.Dictionary(data)\n",
    "dictionary.filter_extremes(no_below=5, no_above=.5, keep_n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace dataset with numeric ids words in vocab (and exclude all other words)\n",
    "corpus = [dictionary.doc2bow(text) for text in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=num_topics, \n",
    "                                           passes=10,\n",
    "                                           alpha='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_topics):\n",
    "    print(\"topic %s:\\t%s\" % (i, ' '.join([term for term, freq in lda_model.show_topic(i, topn=10)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
