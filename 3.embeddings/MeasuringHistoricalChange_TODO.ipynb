{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The meaning of words often changes over time.  In this homework, you will explore this phenomenon by identifying shifts in word meaning over the space of one hundred years by examining word embeddings trained on historical data (largely published before 1923) and those trained on contemporary texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "import operator\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wiki = KeyedVectors.load_word2vec_format(\"../data/glove.6B.50d.50K.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "guten = KeyedVectors.load_word2vec_format(\"../data/gutenberg.200.vectors.50K.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Before we jump in, select 5 words whose senses you believe have changed over the period of the past 100 years. Ensure they are in the vocabulary of both models.  Explain the two different meanings they have.  This is an important step in stating your beliefs before you examine any empirical evidence; do not change these terms after you have run the models you develop below.  (Here we are only evaluating the rationales, not whether the terms *actually* undergo sense change, as measured below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fill in terms here\n",
    "terms=['google', 'apple', 'mouse', 'chip', 'windows']\n",
    "for term in terms:\n",
    "    if term not in wiki or term not in guten:\n",
    "        print(\"%s missing!\" % term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1 response**.\n",
    "\n",
    "The words I've chosen follow a similar line of reasoningâ€”each of them has acquired an additional layer of meaning due to technological advancements. The term `google` was originally coined in the 1930s but only became widely known to the general public with the company's debut. The rest of the words have all developed multiple meanings for the same reason. Words like `apple` and `mouse` have their original, prototypical meanings of a fruit and an animal, as they were perceived a century ago. So as `chip` and `windows`. However, these terms have evolved to be associated with technological concepts such as `internet` and `web` in contemporary usage.\n",
    "\n",
    "`google`: \n",
    "- Meaning 1: used colloquially as a math term\n",
    "- Meaning 2: tech company trademark, to search\n",
    "\n",
    "`apple`\n",
    "- Meaning 1: fruit\n",
    "- Meaning 2: tech company trademark\n",
    "\n",
    "`mouse`\n",
    "- Meaning 1: rodent animal\n",
    "- Meaning 2: tech device\n",
    "\n",
    "`chip`\n",
    "- Meaning 1: fragments; gambling chips; food\n",
    "- Meaning 2: circuit material\n",
    "\n",
    "`windows`\n",
    "- Meaning 1: opening in the wall\n",
    "- Meaning 2: computer operating system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Find the words that have changed the most by calculating the number of words that overlap in their 50 nearest neighbors.  That is, let $\\mathcal{N}_{guten}(\\textrm{awesome})$ be the 50 nearest neighbors for the word \"awesome\" in the Gutenberg embeddings and $\\mathcal{N}_{wiki}(\\textrm{awesome})$ be the 50 nearest neighbors for \"awesome\" in the Wikipedia embeddings.  Calculate the size of $\\mathcal{N}_{guten}(\\textrm{awesome}) \\cap \\mathcal{N}_{wiki}(\\textrm{awesome})$.  Under this method, the words that share the *fewest* neighbors have moved the furthest apart.  Display the 100 words that have moved the furthest apart and the 100 words that have remained the closest together, along with their intersection score.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_words(vocab):\n",
    "    \n",
    "    # initialize a dict to hold the size of overlapped neighbors\n",
    "    num = {}\n",
    "    \n",
    "    # loop through the vocab \n",
    "    for word in vocab: \n",
    "        \n",
    "        # get the number of neighbors in guten \n",
    "        nguten = [k for k, v in guten.most_similar(word, topn=50)]\n",
    "        \n",
    "        # get the number of neighbors in wiki\n",
    "        nwiki = [k for k, v in wiki.most_similar(word, topn=50)]\n",
    "        \n",
    "        # get the overlapped neighbors\n",
    "        overlap = list(set(nguten) & set(nwiki))\n",
    "        \n",
    "        # get the size of overlapping\n",
    "        num[word] = len(overlap)\n",
    "    \n",
    "    # sort the dict by the size of overlapping\n",
    "    sorted_num = dict(sorted(num.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    return sorted_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mouse': 9, 'windows': 7, 'google': 1, 'apple': 1, 'chip': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_words(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the shared vocab between two word lists\n",
    "wiki_vocab = wiki.index_to_key\n",
    "guten_vocab = guten.index_to_key\n",
    "shared_vocab = list(set(wiki_vocab) & set(guten_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the neighbor size score\n",
    "overlaps = find_words(shared_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('38', 44),\n",
       " ('39', 44),\n",
       " ('37', 44),\n",
       " ('49', 43),\n",
       " ('48', 43),\n",
       " ('43', 43),\n",
       " ('59', 42),\n",
       " ('42', 41),\n",
       " ('46', 41),\n",
       " ('41', 41),\n",
       " ('36', 41),\n",
       " ('33', 38),\n",
       " ('6', 38),\n",
       " ('57', 37),\n",
       " ('32', 36),\n",
       " ('1869', 35),\n",
       " ('5', 35),\n",
       " ('55', 35),\n",
       " ('44', 35),\n",
       " ('1866', 34),\n",
       " ('7', 34),\n",
       " ('65', 34),\n",
       " ('1843', 34),\n",
       " ('1865', 34),\n",
       " ('45', 34),\n",
       " ('62', 34),\n",
       " ('56', 33),\n",
       " ('47', 33),\n",
       " ('1844', 33),\n",
       " ('1856', 33),\n",
       " ('2', 33),\n",
       " ('9', 33),\n",
       " ('8', 33),\n",
       " ('1854', 32),\n",
       " ('67', 32),\n",
       " ('1850', 32),\n",
       " ('52', 32),\n",
       " ('35', 32),\n",
       " ('68', 32),\n",
       " ('1902', 32),\n",
       " ('61', 32),\n",
       " ('1831', 31),\n",
       " ('10', 31),\n",
       " ('fifteen', 31),\n",
       " ('1840', 31),\n",
       " ('1907', 31),\n",
       " ('53', 31),\n",
       " ('1858', 31),\n",
       " ('4', 31),\n",
       " ('1845', 31),\n",
       " ('34', 31),\n",
       " ('14', 31),\n",
       " ('1899', 30),\n",
       " ('16', 30),\n",
       " ('fourteen', 30),\n",
       " ('1855', 30),\n",
       " ('21', 30),\n",
       " ('1861', 30),\n",
       " ('1829', 30),\n",
       " ('1908', 30),\n",
       " ('1859', 30),\n",
       " ('11', 30),\n",
       " ('31', 30),\n",
       " ('13', 30),\n",
       " ('77', 30),\n",
       " ('19', 30),\n",
       " ('63', 29),\n",
       " ('1', 29),\n",
       " ('54', 29),\n",
       " ('12th', 29),\n",
       " ('1876', 29),\n",
       " ('kentucky', 29),\n",
       " ('iowa', 29),\n",
       " ('17', 29),\n",
       " ('40', 29),\n",
       " ('1901', 29),\n",
       " ('1903', 29),\n",
       " ('13th', 29),\n",
       " ('1905', 29),\n",
       " ('1885', 29),\n",
       " ('thirteenth', 29),\n",
       " ('3', 29),\n",
       " ('9th', 28),\n",
       " ('1839', 28),\n",
       " ('1870', 28),\n",
       " ('20', 28),\n",
       " ('6th', 28),\n",
       " ('79', 28),\n",
       " ('1864', 28),\n",
       " ('1830', 28),\n",
       " ('15', 28),\n",
       " ('15th', 28),\n",
       " ('1867', 28),\n",
       " ('12', 28),\n",
       " ('1837', 28),\n",
       " ('78', 28),\n",
       " ('18', 28),\n",
       " ('14th', 28),\n",
       " ('twelve', 28),\n",
       " ('25', 28)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 100 words that remained close together\n",
    "list(overlaps.items())[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mop', 0),\n",
       " ('wallop', 0),\n",
       " ('ter', 0),\n",
       " ('painless', 0),\n",
       " ('dimming', 0),\n",
       " ('kennard', 0),\n",
       " ('curt', 0),\n",
       " ('spanning', 0),\n",
       " ('aggravated', 0),\n",
       " ('mitigating', 0),\n",
       " ('snapshot', 0),\n",
       " ('gwen', 0),\n",
       " ('healer', 0),\n",
       " ('flagship', 0),\n",
       " ('mansoor', 0),\n",
       " ('ilk', 0),\n",
       " ('messed', 0),\n",
       " ('bracken', 0),\n",
       " ('reflective', 0),\n",
       " ('bran', 0),\n",
       " ('valdez', 0),\n",
       " ('corliss', 0),\n",
       " ('tenths', 0),\n",
       " ('md', 0),\n",
       " ('countenance', 0),\n",
       " ('dike', 0),\n",
       " ('lowe', 0),\n",
       " ('tagging', 0),\n",
       " ('e-mail', 0),\n",
       " ('rajah', 0),\n",
       " ('lombardo', 0),\n",
       " ('britton', 0),\n",
       " ('include', 0),\n",
       " ('recurrent', 0),\n",
       " ('delano', 0),\n",
       " ('spades', 0),\n",
       " ('decimal', 0),\n",
       " ('seppi', 0),\n",
       " ('transvaal', 0),\n",
       " ('milking', 0),\n",
       " ('doris', 0),\n",
       " ('payne', 0),\n",
       " ('breasted', 0),\n",
       " ('unseated', 0),\n",
       " ('patriarch', 0),\n",
       " ('multiplicity', 0),\n",
       " ('maneuver', 0),\n",
       " ('inexplicably', 0),\n",
       " ('promoter', 0),\n",
       " ('beryl', 0),\n",
       " ('thome', 0),\n",
       " ('dexter', 0),\n",
       " ('retainer', 0),\n",
       " ('compatriots', 0),\n",
       " ('dictating', 0),\n",
       " ('romney', 0),\n",
       " ('exploited', 0),\n",
       " ('steers', 0),\n",
       " ('nimrod', 0),\n",
       " ('scorpion', 0),\n",
       " ('garner', 0),\n",
       " ('doled', 0),\n",
       " ('nouveau', 0),\n",
       " ('teddy', 0),\n",
       " ('gorges', 0),\n",
       " ('westlake', 0),\n",
       " ('sepa', 0),\n",
       " ('workshop', 0),\n",
       " ('impeding', 0),\n",
       " ('rowley', 0),\n",
       " ('mea', 0),\n",
       " ('ber', 0),\n",
       " ('leander', 0),\n",
       " ('statistical', 0),\n",
       " ('cocky', 0),\n",
       " ('christy', 0),\n",
       " ('stresses', 0),\n",
       " ('aldershot', 0),\n",
       " ('clubhouse', 0),\n",
       " ('stroller', 0),\n",
       " ('thrift', 0),\n",
       " ('bygone', 0),\n",
       " ('strang', 0),\n",
       " ('aliens', 0),\n",
       " ('alchemy', 0),\n",
       " ('lagging', 0),\n",
       " ('guise', 0),\n",
       " ('marathon', 0),\n",
       " ('lilian', 0),\n",
       " ('maw', 0),\n",
       " ('weaves', 0),\n",
       " ('stealth', 0),\n",
       " ('josef', 0),\n",
       " ('ration', 0),\n",
       " ('brunt', 0),\n",
       " ('porcupine', 0),\n",
       " ('crockett', 0),\n",
       " ('licks', 0),\n",
       " ('cob', 0),\n",
       " ('aggregate', 0)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 100 words whose meanings shist furthest apart\n",
    "list(overlaps.items())[-100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at how much the candidate terms you defined above have changed their meaning as measured in these embeddings.  First, we can just print their neighborhoods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_top(word):\n",
    "    print(\"=== %s ===\\n\" % word)\n",
    "    print(\"Gutenberg:\")\n",
    "    for k, v in guten.most_similar(word, topn=10):\n",
    "        print(\"%.3f\\t%s\" % (v,k))\n",
    "\n",
    "    print()\n",
    "    print(\"Wikipedia:\")\n",
    "    for k, v in wiki.most_similar(word, topn=10):\n",
    "        print(\"%.3f\\t%s\" % (v,k)) \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google ===\n",
      "\n",
      "Gutenberg:\n",
      "0.542\tmiscellanies\n",
      "0.533\tselections\n",
      "0.525\tscans\n",
      "0.520\tunderline\n",
      "0.516\tdisclaims\n",
      "0.512\tderivable\n",
      "0.503\tepiblast\n",
      "0.499\tmultiple\n",
      "0.496\tetexts\n",
      "0.491\ttranscribed\n",
      "\n",
      "Wikipedia:\n",
      "0.894\tyahoo\n",
      "0.853\taol\n",
      "0.845\tmicrosoft\n",
      "0.818\tinternet\n",
      "0.818\tweb\n",
      "0.809\tfacebook\n",
      "0.793\tebay\n",
      "0.791\tnetscape\n",
      "0.791\tonline\n",
      "0.782\tsoftware\n",
      "\n",
      "=== apple ===\n",
      "\n",
      "Gutenberg:\n",
      "0.686\tfruit\n",
      "0.679\tapples\n",
      "0.662\tapricot\n",
      "0.662\tonion\n",
      "0.661\tpear\n",
      "0.656\tcabbage\n",
      "0.656\tcherry\n",
      "0.656\tpeach\n",
      "0.648\tbread-fruit\n",
      "0.639\tgum\n",
      "\n",
      "Wikipedia:\n",
      "0.754\tblackberry\n",
      "0.744\tchips\n",
      "0.743\tiphone\n",
      "0.733\tmicrosoft\n",
      "0.733\tipad\n",
      "0.722\tpc\n",
      "0.720\tipod\n",
      "0.719\tintel\n",
      "0.715\tibm\n",
      "0.709\tsoftware\n",
      "\n",
      "=== mouse ===\n",
      "\n",
      "Gutenberg:\n",
      "0.701\tkitten\n",
      "0.694\tcat\n",
      "0.641\tdog\n",
      "0.589\tbird\n",
      "0.580\tcaterpillar\n",
      "0.576\tpuppy\n",
      "0.559\tbutterfly\n",
      "0.549\then\n",
      "0.545\tsquirrel\n",
      "0.537\tdormouse\n",
      "\n",
      "Wikipedia:\n",
      "0.797\tmonkey\n",
      "0.781\tbugs\n",
      "0.773\tcat\n",
      "0.762\trabbit\n",
      "0.750\tworm\n",
      "0.731\tclone\n",
      "0.727\trobot\n",
      "0.720\tspider\n",
      "0.710\tbug\n",
      "0.703\tfrog\n",
      "\n",
      "=== chip ===\n",
      "\n",
      "Gutenberg:\n",
      "0.594\tfireman\n",
      "0.585\tcal\n",
      "0.578\tbarkeeper\n",
      "0.568\tjohnny\n",
      "0.562\tdenson\n",
      "0.556\therder\n",
      "0.544\tbrackton\n",
      "0.538\tapplehead\n",
      "0.538\tdisgustedly\n",
      "0.535\tdispiritedly\n",
      "\n",
      "Wikipedia:\n",
      "0.856\tchips\n",
      "0.749\tintel\n",
      "0.749\telectronics\n",
      "0.731\tsemiconductor\n",
      "0.716\tmaker\n",
      "0.708\tcomputer\n",
      "0.707\tmicroprocessor\n",
      "0.703\tmakers\n",
      "0.701\tmicro\n",
      "0.691\tmanufacturing\n",
      "\n",
      "=== windows ===\n",
      "\n",
      "Gutenberg:\n",
      "0.759\tcasements\n",
      "0.753\tdoors\n",
      "0.725\tmullioned\n",
      "0.721\tunglazed\n",
      "0.720\tlatticed\n",
      "0.719\tuncurtained\n",
      "0.712\tshutters\n",
      "0.706\tpanes\n",
      "0.704\tgalleries\n",
      "0.699\twindow\n",
      "\n",
      "Wikipedia:\n",
      "0.801\twindow\n",
      "0.768\tconsole\n",
      "0.747\tdesktop\n",
      "0.745\tdoors\n",
      "0.744\tportable\n",
      "0.726\txp\n",
      "0.712\tplatforms\n",
      "0.708\tinstalling\n",
      "0.707\thardware\n",
      "0.707\troof\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for term in terms:\n",
    "    print_top(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**check+**. Let's make this a little more precise.  Rank all terms by the overlap score you created above, so that words with scores closer to 0 (i.e., no overlap in nearest neighbors) are ranked higher (i.e., closer to position 1). Measure how good your guesses were by calculating their [mean reciprocal rank](https://en.wikipedia.org/wiki/Mean_reciprocal_rank) within this list.  (Again, we're not evaluating how good your guesses were above, but rather the correctness of your implementation of MRR.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38           26507.0\n",
       "39           26507.0\n",
       "37           26507.0\n",
       "49           26504.0\n",
       "48           26504.0\n",
       "              ...   \n",
       "porcupine        1.0\n",
       "crockett         1.0\n",
       "licks            1.0\n",
       "cob              1.0\n",
       "aggregate        1.0\n",
       "Length: 26509, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rank them by the score (the minimum is ranked 1st)\n",
    "pd.Series(overlaps).rank(method='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the inverse of the rank\n",
    "reciprocal_ranks = (1/pd.Series(overlaps).rank(method='min')).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reciprocal rank of the word: google: 0.0001462629808395495\n",
      "The reciprocal rank of the word: apple: 0.0001462629808395495\n",
      "The reciprocal rank of the word: mouse: 4.24881033310673e-05\n",
      "The reciprocal rank of the word: chip: 1.0\n",
      "The reciprocal rank of the word: windows: 4.5964331678617394e-05\n"
     ]
    }
   ],
   "source": [
    "# get the reciprocal ranks of selected terms\n",
    "for word in terms:\n",
    "    rr = reciprocal_ranks[word]\n",
    "    print(f'The reciprocal rank of the word: {word}: {rr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean reciprocal rank of selected terms is 0.20007619567933776\n"
     ]
    }
   ],
   "source": [
    "mrr = np.mean([reciprocal_ranks[word] for word in terms])\n",
    "print(f'The mean reciprocal rank of selected terms is {mrr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MRR of selected terms is close to 0.2, suggesting that overall there's very little overlap in the nearst neighbors. The meaning of the selected words do undergo change over the century. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
