{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "902a9eb8",
   "metadata": {},
   "source": [
    "In this homework, you will debias word embeddings using the method from [Bolukbasi et al. 2016](https://arxiv.org/abs/1607.06520) and interpreted through [Vargas and Cotterell 2020](https://arxiv.org/abs/2009.09435). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3169da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d7827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = KeyedVectors.load_word2vec_format(\"../data/glove.6B.100d.100K.w2v.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bb9ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's print one sample vector just to see what it looks like\n",
    "print(glove[\"man\"].shape)\n",
    "print(glove[\"man\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932ca67b",
   "metadata": {},
   "source": [
    "Now let's calculate the cosine similarity of that vector (\"man\") with a set of other vectors (\"king\" and \"cabbage\").  This returns two cosine similarities, the first cos(man, king) and the second cos(man, cabbage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec789b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove.cosine_similarities(glove[\"man\"], [glove[\"king\"], glove[\"cabbage\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aea7941",
   "metadata": {},
   "source": [
    "Let's use that machinery to find the differences between \"man\" and \"woman\" and a set of target terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26939690",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets=[\"doctor\", \"nurse\", \"actor\", \"actress\", \"mechanic\", \"librarian\", \"architect\", \"magician\", \"cook\", \"chef\"]\n",
    "diffs={}\n",
    "for term in targets:\n",
    "    \n",
    "    m,w=glove.cosine_similarities(glove[term], [glove[\"man\"], glove[\"woman\"]])\n",
    "    diffs[term]=m-w\n",
    "\n",
    "for k, v in sorted(diffs.items(), key=lambda item: item[1], reverse=True):\n",
    "    print(\"%.3f\\t%s\" % (v,k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6085ba0",
   "metadata": {},
   "source": [
    "We can see a gender difference here, where \"man\" is more aligned \"magician\" and \"mechanic\" and \"woman\" is more aligned with \"actress\" and \"nurse\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881bf91a",
   "metadata": {},
   "source": [
    "**Q1.** Let's debias those embeddings, using the method from [Bolukbasi et al. 2016](https://arxiv.org/abs/1607.06520) and interpreted through [Vargas and Cotterell 2020](https://arxiv.org/abs/2009.09435).  Debiasing embeddings requires two steps: finding the gender subspace and then subtracting the orthogonal projection onto that subspace from the original embedding.  Let's start with the first step: creating \"defining sets\" that capture the variation:\n",
    "\n",
    "$$\n",
    "D_1 = \\{man, woman\\}\\\\\n",
    "D_2 = \\{mr., mrs.\\}\n",
    "$$\n",
    "\n",
    "Following Vargas and Cotterell, we can find the gender subspace by constructing a new matrix $D'$ by substracting the embedding for a word in a defining set from the average of embeddings in that set. Using $e_{word}$ to denote the embedding for a word, this process would results in the following for the defining sets above:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "e_{man} - \\textrm{mean}(e_{man},e_{woman}) \\\\\n",
    "e_{woman} - \\textrm{mean}(e_{man},e_{woman})\\\\\n",
    "e_{mr.} - \\textrm{mean}(e_{mr.},e_{mrs.})\\\\\n",
    "e_{mrs.} - \\textrm{mean}(e_{mr.},e_{mrs.})\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If the original embeddings (e.g., for $e_{man}$) are 100 dimensions (and so the mean over any set of embeddings is also 100 dimensions), then the resulting matrix $D'$ should be $4 \\times 100$.  Create this matrix $D'$ and name it `subspace_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7abe9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "D1=[\"man\", \"woman\"]\n",
    "D2=[\"mr.\", \"mrs.\"]\n",
    "\n",
    "# TODO   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61373e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should be (4,100)\n",
    "print(subspace_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6274b5c3",
   "metadata": {},
   "source": [
    "Step two is to run [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) over that `subspace_matrix` matrix.  The gender subspace in this example is the first principle component of that process. Here's how you run PCA on a random matrix to get the first principle component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc85869",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_matrix=np.random.rand(3,3)\n",
    "print(\"fake matrix:\")\n",
    "print(fake_matrix)\n",
    "\n",
    "# We only need one principle component, so we'll set n_components=1\n",
    "pca=PCA(n_components=1).fit(fake_matrix)\n",
    "subspace=pca.components_[0]\n",
    "\n",
    "print(\"first principle component:\")\n",
    "print(subspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f9c85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You'll see that this subspace is already normalized to unit length:\n",
    "print(subspace)\n",
    "print(subspace/np.sqrt(np.dot(subspace, subspace)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b2c16c",
   "metadata": {},
   "source": [
    "**Q2.** Run PCA on that subspace matrix to get the subspace axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2462a09c",
   "metadata": {},
   "source": [
    "That subspace is the gender axis. You'll remember from class that we find the orthogoal projection of any unit-normalized vector $w$ onto a subspace $b$ by:\n",
    "\n",
    "$$\n",
    "w_b = \\textrm{dot}(w,b) \\; b\n",
    "$$\n",
    "\n",
    "If $b$ and $x$ are 100 dimensions, $w_b$ is 100 dimensions too.  The debiased vector $w_d$ is then simply $w - w_b$.  \n",
    "\n",
    "**Q3.** Debias the vectors for \"man\", \"woman\", and the targets used above (\"doctor\", \"nurse\", \"actor\", \"actress\", \"mechanic\", \"librarian\", \"architect\", \"magician\", \"cook\", \"chef\") and see if debiasing changes the differences between these terms and \"man\"/\"woman\" as noted above.  Glove embeddings are not normalized ahead of time, so be sure to normalize them before carrying out your projection (i.e., dividing vector v by $\\sqrt{\\textrm{dot}(v,v)}$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22018d7",
   "metadata": {},
   "source": [
    "**check-plus**. Reflect in 100 words on the differences between this gender axis construction and the axis construction in SemAxis.  How are they different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523d6a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
