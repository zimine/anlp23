{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In a previous homework, you constructed semantic axis using static word embeddings, defining a continuum for a particular concept (rich-poor, sad-happy, etc.) that you can then situate *any* word along.  As we've discussed, one issue with static embeddings is lexical *polysemy*---static embeddings must encode *all* meanings of a word, and a word like \"rich\" has many other senses beyond describing wealth (e.g., \"rich dessert\").  In this homework, you'll address this by constructing *contextual* semantic axes."
      ],
      "metadata": {
        "id": "cuCpZP1rg-6t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This homework is inspired by methods in the following paper (read it to get a sense of what contextual vectors can be used for, and also as an example to guide your final project reports): Lucy et al (2022), [Discovering Differences in the Representation of People using Contextualized Semantic Axes](https://aclanthology.org/2022.emnlp-main.228/)."
      ],
      "metadata": {
        "id": "qG-4moOYh5Uo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8nJl82YcB7r"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yFIldYSGg-M5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "RDqlbpOxcF1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use `bert-base` on Colab; if you're not able to run on Colab, feel free to use the smaller BERT models we discussed in class."
      ],
      "metadata": {
        "id": "tjZ6XsWsA_pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "lxlZLONocHIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some functions that may be useful."
      ],
      "metadata": {
        "id": "p1puM0HdA8r8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))"
      ],
      "metadata": {
        "id": "izMrRV3kcJzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bert_for_token(string, term):\n",
        "\n",
        "    # tokenize\n",
        "    inputs = tokenizer(string, return_tensors=\"pt\")\n",
        "\n",
        "    # convert input ids to words\n",
        "    tokens=tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "\n",
        "    print(tokens)\n",
        "\n",
        "    # find the first location of the query term among those tokens (so we know which BERT rep to use)\n",
        "    term_idx=tokens.index(term)\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # return the BERT rep for that token index\n",
        "    # The output is a pytorch tensor object, but let's convert it to a numpy object to work with numpy functions\n",
        "\n",
        "    return outputs.last_hidden_state[0][term_idx].detach().numpy()\n"
      ],
      "metadata": {
        "id": "t0e90pS0cXeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_rep=get_bert_for_token(\"I ate some jam with toast\", \"jam\")\n",
        "print(query_rep.shape)"
      ],
      "metadata": {
        "id": "Jj9Z79fAcZIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With static semantic axes, you defined the endpoints by choosing sets of words (e.g., {happy, elated} vs. {sad, unhappy}).  Here you will define them with sentences that contain the term (e.g., \"It was Saturday morning and John woke up **happy**.\")  \n",
        "\n",
        "**Q1**. Describe the concept for your contextual semantic axis and create 5 sentences that describe the two endpoints (i.e., 10 sentences total)."
      ],
      "metadata": {
        "id": "w9PAyHZNiTNb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cL7r_NDXiSO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2**. Now you will create your semantic axis.  Use the `get_bert_for_token` code above to get the BERT embedding for the target word in each of your sentences.  Create a positive vector $V^+$ as the average for all the BERT embeddings for one class, and a negative vector $V^-$ as the average for all the BERT embeddings for the second class.  Following the SemAxis structure, your axis is then $V_\\textrm{axis}=V^+-V^-$.  If you are using BERT base, your axis should be a 768-dimensional vector."
      ],
      "metadata": {
        "id": "thj5-ucVjGyA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hMdizCoXjPvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3**.  Now select 5 terms **in context** that you want to situate along that axis. For each example, construct a sentence containing a use of that term, get its BERT embedding, and take its cosine similarity with the axis you've defined.  E.g., for a term \"wicked\" in a context senntence \"That homework is **wicked** hard\", you will take the BERT embedding for \"wicked\" in that sentence to situate along the axis you've defined above.\n",
        "\n",
        "$$\n",
        "\\textrm{score}= \\textrm{cos}(\\textrm{wicked}_\\textrm{That homework is wicked hard}, V_\\textrm{axis})\n",
        "$$"
      ],
      "metadata": {
        "id": "gYy_zzDmjfAK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G51Y2xsLjqNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**check-plus**.  You've now situated words in context along a semantic axis. How would you know if this method is accurate or not?  In 200 words, brainstorm an evaluation that you could carry out. Be specific enough that you provide enough detail for us to carry it out from your description alone."
      ],
      "metadata": {
        "id": "noGmKd2M9xxd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "14-4R_ys-Cki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To turn in:\n",
        "\n",
        "- Go to `File > Download > Download .ipynb` and save your notebook.\n",
        "- In your browser, print this page to save as PDF.\n",
        "- Upload both your .ipynb and .pdf files to bCourses as usual."
      ],
      "metadata": {
        "id": "DTyUkJv0ozlf"
      }
    }
  ]
}