{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this homework, you'll explore using BERT and SentenceTransformers into the Lesk algorithm for word sense disambiguation.  (You'll likely want to run this on Colab.)"
      ],
      "metadata": {
        "id": "WEY7WCelxO7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "kk0DArdfERk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsVxXQYqENqw"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "import numpy as np\n",
        "from nltk.corpus import wordnet as wn\n",
        "import nltk\n",
        "from scipy.spatial.distance import cosine\n",
        "import operator\n",
        "import torch\n",
        "from math import sqrt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "WxgpGSoIEiTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qM79IHOsENq0"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdjNfGkpENq1"
      },
      "source": [
        "**Q1:** The Lesk algorithm we discussed in class uses information about the *context* around a term in calculating the similarity between a word in a sentence and a word in a dictionary gloss.  For instance, [Basile et al. 2014](https://www.aclweb.org/anthology/C/C14/C14-1151.pdf) use static word vectors to provide this context-level information, where we measure the similarity between a gloss g = $\\{ g_1, \\ldots, g_G \\}$ and context c = $\\{ c_1, \\ldots, c_C \\}$ as the cosine similarity between the sum of distributed representations:\n",
        "\n",
        "$$\n",
        "\\cos \\left(\\sum_{i=1}^G g_i, \\sum_{i=1}^C c_i  \\right)\n",
        "$$\n",
        "\n",
        "However, over the past few weeks we've considered how contextual language models like BERT already provide a sentence-level contextualization for words.  So given a target sentence (\"I withdrew money from the *bank*\") with target term (bank), and a list of dictionary glosses/examples corresponding to different senses (\"A bank is a financial institution\" = bank1; \"A bank is the side of a river\" = bank2), let's adapt the Lesk algorithm to simply calculate the similarity between the average BERT embedding for all words in the target sentence (including the [CLS] and [SEP] tokens) and the average BERT embedding for all the words in the sense gloss (again including [CLS] and [SEP]):\n",
        "\n",
        "$$\n",
        "\\cos \\left({1 \\over G}\\sum_{i=1}^G BERT(g_{i}), {1 \\over C} \\sum_{j=1}^C BERT(c_{j}) \\right)\n",
        "$$\n",
        "\n",
        "\n",
        "* The gloss for a synset can be found in `synset.definition()`.\n",
        "* You can find the cosine similarity between two vectors below.\n",
        "* `wn.synsets(word, pos=part_of_speech)` gets you a list of the synsets for a word with a specific part of speech (e.g., \"n\" for noun)\n",
        "* Feel free to draw on the code you've already seen for getting the BERT embeddings for words (e.g., `3.embeddings/BERT.ipynb`)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(vec1, vec2):\n",
        "  return np.dot(vec1, vec2)/(sqrt(np.dot(vec1, vec1)) * sqrt(np.dot(vec2, vec2)))"
      ],
      "metadata": {
        "id": "bpOZ0PgEHSEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nELpuK-YENq3"
      },
      "outputs": [],
      "source": [
        "def bert_lesk(word, sentence, part_of_speech):\n",
        "\n",
        "    def get_bert_for_token(string, term):\n",
        "\n",
        "        # your code here\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    context_vector=get_bert_for_token(sentence, word)\n",
        "\n",
        "    synsets=...\n",
        "\n",
        "    vals={}\n",
        "    for synset in synsets:\n",
        "        vector=get_bert_for_token(...)\n",
        "        vals[synset]=cosine_similarity(...)\n",
        "\n",
        "    sorted_x = sorted(vals.items(), key=operator.itemgetter(1), reverse=True)\n",
        "    for k,v in sorted_x:\n",
        "        print(\"%.3f\\t%s\\t%s\"% (v,k,k.definition()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD7-x3uMENq3"
      },
      "source": [
        "Execute the following two cells to check whether your implementation distinguishes between these two senses of \"bank\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bixGcGfIENq4"
      },
      "outputs": [],
      "source": [
        "bert_lesk(\"bank\", \"I deposited my money into my savings account at the bank\", \"n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZFupiPhENq4"
      },
      "outputs": [],
      "source": [
        "bert_lesk(\"bank\", \"I ran along the river bank\", \"n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2.  Now do the same thing with SentenceBERT.  For a gloss $g$ and a target sentence $c$ containing the word to disambiguate, calculate the similarity between them as the cosine similarity of the SentenceBERT vectors of each one:\n",
        "\n",
        "$$\n",
        "\\cos \\left(\\textrm{SBERT}(g), \\textrm{SBERT}(c) \\right)\n",
        "$$\n"
      ],
      "metadata": {
        "id": "7BRi7d3tFBf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "sentence_model = SentenceTransformer('sentence-transformers/all-distilroberta-v1')"
      ],
      "metadata": {
        "id": "dQq5RCDlGSDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jt0Cue_uENq5"
      },
      "outputs": [],
      "source": [
        "def sentencebert_lesk(word, sentence, part_of_speech):\n",
        "\n",
        "    # your code here\n",
        "\n",
        "    vals={}\n",
        "\n",
        "    # your code here\n",
        "\n",
        "    sorted_x = sorted(vals.items(), key=operator.itemgetter(1), reverse=True)\n",
        "    for k,v in sorted_x:\n",
        "        print(\"%.3f\\t%s\\t%s\"% (v,k,k.definition()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute the following two cells to check whether your implementation of SentenceBERT-Lesk distinguishes between these two senses of \"bank\"."
      ],
      "metadata": {
        "id": "hmodD40PIXX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentencebert_lesk(\"bank\", \"I deposited my money into my savings account at the bank\", \"n\")"
      ],
      "metadata": {
        "id": "sZa-R20JGeAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentencebert_lesk(\"bank\", \"I ran along the river bank\", \"n\")"
      ],
      "metadata": {
        "id": "gJXuzOvyGfvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HaNYTxWtIGpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BzQX1rQDu_Og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To turn in:\n",
        "\n",
        "- Go to `File > Download > Download .ipynb` and save your notebook.\n",
        "- In your browser, print this page to save as PDF.\n",
        "- Upload both your .ipynb and .pdf files to bCourses as usual."
      ],
      "metadata": {
        "id": "UN5aUo0Eu9P1"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}