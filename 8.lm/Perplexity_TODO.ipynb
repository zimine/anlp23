{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4c35ba5",
   "metadata": {},
   "source": [
    "In this homework, you'll explore perplexity as a measure of evaluation for language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b52b3a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "from collections import Counter\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33eb376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    sequences=[]\n",
    "    with open(filename) as file:\n",
    "        data=file.read()\n",
    "        sents=sent_tokenize(data)\n",
    "        for sent in sents:\n",
    "            tokens=word_tokenize(sent)\n",
    "            sequences.append(tokens)\n",
    "            \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feeb46f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from file and tokenize them into sequences comprised of tokens.\n",
    "\n",
    "# Pride and Prejudice (Jane Austen)\n",
    "sequences=read_file(\"../data/stylometry/1342_pride_and_prejudice.txt\")\n",
    "\n",
    "max_sequences=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "214dbd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramModel():\n",
    "\n",
    "    def __init__(self, sequences, order):\n",
    "        \n",
    "        # For this exercise we're going to encode the LM as a sparse dictionary (training less storage for more compute)\n",
    "        # We'll store the LM as a dictionary with the conditioning context as keys; each value is a \n",
    "        # Counter object that keeps track of the number of times we see a word following that context.\n",
    "        \n",
    "        self.counts={}\n",
    "        \n",
    "        # Markov order (order 1 = conditioning on previous 1 word; order 2 = previous 2 words, etc.)\n",
    "        self.order=order\n",
    "        \n",
    "        vocab={\"[END]\":0}\n",
    "                \n",
    "        for s_idx, tokens in enumerate(sequences):\n",
    "            # We'll add [START] and [END] tokens to encode the beginning/end of sentences\n",
    "            token_copy=copy.deepcopy(tokens)\n",
    "            for i in range(order):\n",
    "                token_copy.insert(0, \"[START]\")\n",
    "            token_copy.append(\"[END]\")\n",
    "            \n",
    "        \n",
    "            for i in range(order, len(token_copy)):\n",
    "                context=\" \".join(token_copy[i-order:i])\n",
    "                word=token_copy[i]\n",
    "                \n",
    "                if word not in vocab:\n",
    "                    vocab[word]=len(vocab)\n",
    "                \n",
    "                # For just the first sentence, print the conditioning context + word\n",
    "                if s_idx == 0:\n",
    "                    print(\"Context: %s Next: %s\" % (context.ljust(50), word))\n",
    "                    \n",
    "                if context not in self.counts:\n",
    "                    self.counts[context]=Counter()\n",
    "                self.counts[context][word]+=1\n",
    "                \n",
    "\n",
    "\n",
    "    def sample(self, context):\n",
    "\n",
    "        total=sum(self.counts[context].values())\n",
    "        \n",
    "        dist=[]\n",
    "        vocab=[]\n",
    "\n",
    "        # Create a probability distribution for each conditioning context, over the vocab that we've observed it with.\n",
    "        for idx, word in enumerate(self.counts[context]):\n",
    "            prob=self.counts[context][word]/total\n",
    "            dist.append(prob)\n",
    "            vocab.append(word)\n",
    "\n",
    "        index=np.argmax(np.random.multinomial(1, pvals=dist))\n",
    "        return vocab[index]\n",
    "        \n",
    "    def generate_sequence(self):\n",
    "        generated=[\"[START]\"]*(self.order)\n",
    "        word=None\n",
    "        while word != \"[END]\":\n",
    "            context=' '.join(generated[-self.order:] if self.order > 0 else \"\")\n",
    "            word=self.sample(context)\n",
    "            print(word)\n",
    "            generated.append(word)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0384b034",
   "metadata": {},
   "source": [
    "Let's create some language models of different orders from *Pride and Prejudice*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4502623",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:                                                    Next: Chapter\n",
      "Context:                                                    Next: 1\n",
      "Context:                                                    Next: It\n",
      "Context:                                                    Next: is\n",
      "Context:                                                    Next: a\n",
      "Context:                                                    Next: truth\n",
      "Context:                                                    Next: universally\n",
      "Context:                                                    Next: acknowledged\n",
      "Context:                                                    Next: ,\n",
      "Context:                                                    Next: that\n",
      "Context:                                                    Next: a\n",
      "Context:                                                    Next: single\n",
      "Context:                                                    Next: man\n",
      "Context:                                                    Next: in\n",
      "Context:                                                    Next: possession\n",
      "Context:                                                    Next: of\n",
      "Context:                                                    Next: a\n",
      "Context:                                                    Next: good\n",
      "Context:                                                    Next: fortune\n",
      "Context:                                                    Next: ,\n",
      "Context:                                                    Next: must\n",
      "Context:                                                    Next: be\n",
      "Context:                                                    Next: in\n",
      "Context:                                                    Next: want\n",
      "Context:                                                    Next: of\n",
      "Context:                                                    Next: a\n",
      "Context:                                                    Next: wife\n",
      "Context:                                                    Next: .\n",
      "Context:                                                    Next: [END]\n"
     ]
    }
   ],
   "source": [
    "ngram0=NgramModel(sequences[:max_sequences], order=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81a51b9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: [START]                                            Next: Chapter\n",
      "Context: Chapter                                            Next: 1\n",
      "Context: 1                                                  Next: It\n",
      "Context: It                                                 Next: is\n",
      "Context: is                                                 Next: a\n",
      "Context: a                                                  Next: truth\n",
      "Context: truth                                              Next: universally\n",
      "Context: universally                                        Next: acknowledged\n",
      "Context: acknowledged                                       Next: ,\n",
      "Context: ,                                                  Next: that\n",
      "Context: that                                               Next: a\n",
      "Context: a                                                  Next: single\n",
      "Context: single                                             Next: man\n",
      "Context: man                                                Next: in\n",
      "Context: in                                                 Next: possession\n",
      "Context: possession                                         Next: of\n",
      "Context: of                                                 Next: a\n",
      "Context: a                                                  Next: good\n",
      "Context: good                                               Next: fortune\n",
      "Context: fortune                                            Next: ,\n",
      "Context: ,                                                  Next: must\n",
      "Context: must                                               Next: be\n",
      "Context: be                                                 Next: in\n",
      "Context: in                                                 Next: want\n",
      "Context: want                                               Next: of\n",
      "Context: of                                                 Next: a\n",
      "Context: a                                                  Next: wife\n",
      "Context: wife                                               Next: .\n",
      "Context: .                                                  Next: [END]\n"
     ]
    }
   ],
   "source": [
    "ngram1=NgramModel(sequences[:max_sequences], order=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35ee1ead",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: [START] [START]                                    Next: Chapter\n",
      "Context: [START] Chapter                                    Next: 1\n",
      "Context: Chapter 1                                          Next: It\n",
      "Context: 1 It                                               Next: is\n",
      "Context: It is                                              Next: a\n",
      "Context: is a                                               Next: truth\n",
      "Context: a truth                                            Next: universally\n",
      "Context: truth universally                                  Next: acknowledged\n",
      "Context: universally acknowledged                           Next: ,\n",
      "Context: acknowledged ,                                     Next: that\n",
      "Context: , that                                             Next: a\n",
      "Context: that a                                             Next: single\n",
      "Context: a single                                           Next: man\n",
      "Context: single man                                         Next: in\n",
      "Context: man in                                             Next: possession\n",
      "Context: in possession                                      Next: of\n",
      "Context: possession of                                      Next: a\n",
      "Context: of a                                               Next: good\n",
      "Context: a good                                             Next: fortune\n",
      "Context: good fortune                                       Next: ,\n",
      "Context: fortune ,                                          Next: must\n",
      "Context: , must                                             Next: be\n",
      "Context: must be                                            Next: in\n",
      "Context: be in                                              Next: want\n",
      "Context: in want                                            Next: of\n",
      "Context: want of                                            Next: a\n",
      "Context: of a                                               Next: wife\n",
      "Context: a wife                                             Next: .\n",
      "Context: wife .                                             Next: [END]\n"
     ]
    }
   ],
   "source": [
    "ngram2=NgramModel(sequences[:max_sequences], order=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c55ac8d",
   "metadata": {},
   "source": [
    "**Q1.** Create a `perplexity` function that can takes two arguments: a.) a model of *any* ngram order (from the class above); and b.) a sequence to calculate perplexity for.  You'll recall from class that perplexity under a particular language model for sequence $w$ is given by the following equation:\n",
    "\n",
    "$$\n",
    "\\textrm{perplexity}_{model}(w) = \\exp\\left(-{1 \\over N} \\sum_{i=1}^N \\log P_{model}(w_i) \\right)\n",
    "$$\n",
    "\n",
    "$P_{model}(w_i)$ calculates the probability of token $w_i$ using whatever assumptions that model makes -- for a bigram model (order 1), this is $P(w_i \\mid w_{i-1})$, for a trigram model (order 2), this is $P(w_i \\mid w_{i-2}, w_{i-1})$, etc.  Two things to note:\n",
    "\n",
    "* When calculating the probability of the first word(s), be sure to get the conditioning context right.  The conditioning context for the first word in a trigram model, for example, is $P(w_i \\mid$ [START] [START]$)$.\n",
    "* Perplexity is only calculated for the words in the actual sequence.  We don't include the $P$([START]) or $P$([END]) in the perplexity calculuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d32301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(model, tokens):\n",
    "    \n",
    "    # get the order of the language model\n",
    "    order = model.order\n",
    "    contexts = {}\n",
    "    \n",
    "    # insert start tokens\n",
    "    for i in range(order):\n",
    "        tokens.insert(0, \"[START]\")\n",
    "    \n",
    "    # create contexts for each token in the sequence\n",
    "    for i in range(order, len(tokens)):\n",
    "        context=\" \".join(tokens[i-order:i])\n",
    "        word=tokens[i]\n",
    "        if context not in contexts:\n",
    "            contexts[context] = word\n",
    "    \n",
    "    # get the prob of each token\n",
    "    probs=0\n",
    "    if order!=0: \n",
    "        assert len(contexts) == len(tokens) - order\n",
    "        for k, v in contexts.items():\n",
    "            total=sum(model.counts[k].values())\n",
    "            target=model.counts[k][v]\n",
    "            logp=np.log(target/total)\n",
    "            probs+=logp\n",
    "        perplexity=np.exp(-probs/len(contexts))\n",
    "        \n",
    "    elif order==0:\n",
    "        total=sum(model.counts[''].values())\n",
    "        for token in tokens:\n",
    "            target=model.counts[''][token]\n",
    "            logp=np.log(target/total)\n",
    "            probs+=logp\n",
    "        perplexity=np.exp(-probs/len(tokens))\n",
    "        \n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8d0a8a",
   "metadata": {},
   "source": [
    "**Q2**. Execute that perplexity function for the following language models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a4b0e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202.22631978521557"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(ngram0, word_tokenize(\"She was a great friend of Mr. Bingley.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd2d4b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.58810289679188"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(ngram1, word_tokenize(\"She was a great friend of Mr. Bingley.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05be627d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.396057825901801"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(ngram2, word_tokenize(\"She was a great friend of Mr. Bingley.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a331078",
   "metadata": {},
   "source": [
    "**Q3.** What is the perplexity of \"She was a really great friend of Mr. Bingley.\" in the trigram language model trained above?  Explain 100 words that behavior is expected (and correct) given how this language model and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0a4f81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lg/ly0nyv616nj7vqtsl1wcyj8c0000gn/T/ipykernel_62989/2922606783.py:25: RuntimeWarning: divide by zero encountered in log\n",
      "  logp=np.log(target/total)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'really great'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mperplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mngram2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mShe was a really great friend of Mr. Bingley.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mperplexity\u001b[0;34m(model, tokens)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contexts) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens) \u001b[38;5;241m-\u001b[39m order\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m contexts\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 23\u001b[0m     total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28msum\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcounts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m     24\u001b[0m     target\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcounts[k][v]\n\u001b[1;32m     25\u001b[0m     logp\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog(target\u001b[38;5;241m/\u001b[39mtotal)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'really great'"
     ]
    }
   ],
   "source": [
    "perplexity(ngram2, word_tokenize(\"She was a really great friend of Mr. Bingley.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df1fe0a-e8f1-405c-bfa1-2c2b07ab6b75",
   "metadata": {},
   "source": [
    "We cannot compute the perplexity of `She was a really great friend of Mr. Binley` in the trigram model because the context `really great` does not appear in the training set. We can verify this by printing out any bigrams starting with the token `really`, and `really great` is not one of the bigrams that the trigram has seen. This behavior is expected as a ngram model assigns probabilities to a test set based on the training set. If the training set is sufficiently representative, it should assign high probabilities to the test set, should it be well-constructed. Perplexity as an intrinsic eval measure applies to the model itself; if the model has the issue of sparsity, either because the context or the token is not seen by the model, it is reflected in the calculation of perplexity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77ba550e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'really great'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mngram2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcounts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreally great\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'really great'"
     ]
    }
   ],
   "source": [
    "ngram2.counts[\"really great\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6d17209-955b-40a9-a92b-1d9366f79add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram1.counts[\"really\"][\"great\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
