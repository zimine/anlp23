{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores a simple hypothesis test checking whether the accuracy of a trained model for binary classification is meaningfully different from a majority class baseline.  We test this making a parametric assumption: we assume that the binary correct/incorrect results follow a binomial distribution (and approximate the binomial with a normal distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "from math import sqrt \n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            label=cols[0]\n",
    "            text=cols[1]\n",
    "            X.append(text)\n",
    "            Y.append(label)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The directory should contain train.tsv, dev.tsv and test.tsv\n",
    "directory=\"../data/lmrd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainX, trainY=read_data(\"%s/train.tsv\" % directory)\n",
    "devX, devY=read_data(\"%s/dev.tsv\" % directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def majority_class(trainY, devY):\n",
    "    labelCounts=Counter()\n",
    "    for label in trainY:\n",
    "        labelCounts[label]+=1\n",
    "    majority=labelCounts.most_common(1)[0][0]\n",
    "    \n",
    "    correct=0.\n",
    "    for label in devY:\n",
    "        if label == majority:\n",
    "            correct+=1\n",
    "            \n",
    "    print(\"%s\\t%.3f\" % (majority, correct/len(devY)))\n",
    "    return correct/len(devY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here's a sample dictionary we can create by inspecting the output of the Mann-Whitney test (in 2.compare/)\n",
    "dem_dictionary=set([\"republican\",\"cut\", \"opposition\"])\n",
    "repub_dictionary=set([\"growth\",\"economy\"])\n",
    "\n",
    "def political_dictionary_feature(tokens):\n",
    "    feats={}\n",
    "    for word in tokens:\n",
    "        if word in dem_dictionary:\n",
    "            feats[\"word_in_dem_dictionary\"]=1\n",
    "        if word in repub_dictionary:\n",
    "            feats[\"word_in_repub_dictionary\"]=1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start with a fixed dict to look up keywords occuring in the review\n",
    "pos_dictionary = set([\"like\", \"love\", \"good\"])\n",
    "neg_dictionary = set([\"dislike\", \"hate\", \"bad\"])\n",
    "\n",
    "def sentiment_dictionary_feature(tokens):\n",
    "    feats={}\n",
    "    for word in tokens:\n",
    "        if word in pos_dictionary:\n",
    "            feats[\"word_in_pos_dictionary\"]=1\n",
    "        if word in neg_dictionary:\n",
    "            feats[\"word_in_neg_dictionary\"]=1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add a keyword feature set\n",
    "pos_cap_dict = set([\"WOW\", \"ABSOLUTELY\", \"SO\", \"EVER\"])\n",
    "neg_cap_dict = set([\"NOT\", \"VERY\", \"BUT\", \"?\", \"ONLY\", \"NO\", \"WTF\", \"...\", \"SPENT\", \"$\"])\n",
    "\n",
    "def new_feature_class_two(tokens):\n",
    "    feats={}\n",
    "    for word in tokens:\n",
    "        if word in pos_cap_dict:\n",
    "            feats[\"allcaps_in_pos_dict\"]=1\n",
    "        if word in neg_cap_dict:\n",
    "            feats[\"allcaps_in_neg_dict\"]=1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unigram_feature(tokens):\n",
    "    feats={}\n",
    "    for word in tokens:\n",
    "        feats[\"UNIGRAM_%s\" % word]=1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_features(trainX, feature_functions):\n",
    "    data=[]\n",
    "    for doc in trainX:\n",
    "        feats={}\n",
    "\n",
    "        # sample text data is already tokenized; if yours is not, do so here\n",
    "        tokens=doc.split(\" \")\n",
    "        \n",
    "        for function in feature_functions:\n",
    "            feats.update(function(tokens))\n",
    "\n",
    "        data.append(feats)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This helper function converts a dictionary of feature names to unique numerical ids\n",
    "def create_vocab(data):\n",
    "    feature_vocab={}\n",
    "    idx=0\n",
    "    for doc in data:\n",
    "        for feat in doc:\n",
    "            if feat not in feature_vocab:\n",
    "                feature_vocab[feat]=idx\n",
    "                idx+=1\n",
    "                \n",
    "    return feature_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This helper function converts a dictionary of feature names to a sparse representation\n",
    "# that we can fit in a scikit-learn model.  This is important because almost all feature \n",
    "# values will be 0 for most documents (note: why?), and we don't want to save them all in \n",
    "# memory.\n",
    "\n",
    "def features_to_ids(data, feature_vocab):\n",
    "    new_data=sparse.lil_matrix((len(data), len(feature_vocab)))\n",
    "    for idx,doc in enumerate(data):\n",
    "        for f in doc:\n",
    "            if f in feature_vocab:\n",
    "                new_data[idx,feature_vocab[f]]=doc[f]\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function evaluates a list of feature functions on the training/dev data arguments\n",
    "def pipeline(trainX, devX, trainY, devY, feature_functions):\n",
    "    trainX_feat=build_features(trainX, feature_functions)\n",
    "    devX_feat=build_features(devX, feature_functions)\n",
    "\n",
    "    # just create vocabulary from features in *training* data\n",
    "    feature_vocab=create_vocab(trainX_feat)\n",
    "\n",
    "    trainX_ids=features_to_ids(trainX_feat, feature_vocab)\n",
    "    devX_ids=features_to_ids(devX_feat, feature_vocab)\n",
    "    \n",
    "    logreg = linear_model.LogisticRegression(C=1.0, solver='lbfgs', penalty='l2', max_iter=10000)\n",
    "    logreg.fit(trainX_ids, trainY)\n",
    "    print(\"Accuracy: %.3f\" % logreg.score(devX_ids, devY))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function trains a model and returns the predicted and true labels for test data\n",
    "def evaluate(trainX, devX, trainY, devY, feature_functions):\n",
    "    trainX_feat=build_features(trainX, feature_functions)\n",
    "    devX_feat=build_features(devX, feature_functions)\n",
    "\n",
    "    # just create vocabulary from features in *training* data\n",
    "    feature_vocab=create_vocab(trainX_feat)\n",
    "\n",
    "    trainX_ids=features_to_ids(trainX_feat, feature_vocab)\n",
    "    devX_ids=features_to_ids(devX_feat, feature_vocab)\n",
    "    \n",
    "    logreg = linear_model.LogisticRegression(C=1.0, solver='lbfgs', penalty='l2', max_iter=10000)\n",
    "    logreg.fit(trainX_ids, trainY)\n",
    "    predictions=logreg.predict(devX_ids)\n",
    "    return (predictions, devY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\t0.500\n"
     ]
    }
   ],
   "source": [
    "baseline=majority_class(trainY,devY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def binomial_test(predictions, truth, baseline, significance_level=0.95):\n",
    "    correct=[]\n",
    "    for pred, gold in zip(predictions, truth):\n",
    "        correct.append(int(pred==gold))\n",
    "        \n",
    "    success_rate=np.mean(correct)\n",
    "\n",
    "    # two-tailed test\n",
    "    critical_value=(1-significance_level)/2\n",
    "    # ppf finds z such that p(X < z) = critical_value\n",
    "    z_alpha=-1*norm.ppf(critical_value)\n",
    "    print(\"Critical value: %.3f\\tz_alpha: %.3f\" % (critical_value, z_alpha))\n",
    "    \n",
    "    # the standard error is the square root of the variance/sample size\n",
    "    # the variance for a binomial test is p*(1-p)\n",
    "    standard_error=sqrt((success_rate*(1-success_rate))/len(correct))\n",
    "\n",
    "    Z=(success_rate-baseline)/standard_error\n",
    "    lower=success_rate-z_alpha*standard_error\n",
    "    upper=success_rate+z_alpha*standard_error\n",
    "    pval=norm.cdf(-abs(Z))\n",
    "    print (\"Accuracy: %.3f, n = %s\" % (success_rate, len(correct)))\n",
    "    print(\"%s%% Confidence interval: [%.3f,%.3f]\" % (significance_level*100, lower, upper))\n",
    "\n",
    "    print(\"Z score: %.3f\" % Z)\n",
    "    print(\"p-value: %.5f\" % pval)\n",
    "\n",
    "    print (\"Critical region corresponding to z_alpha=[%.3f,%.3f]: [%.3f, %.3f]\" % (-z_alpha, z_alpha, baseline-z_alpha*standard_error, baseline+z_alpha*standard_error))\n",
    "    print (\"Can we reject null that %.3f is different from %.3f at %s significance level? %s\" % (success_rate, baseline, significance_level*100, \"Yes\" if Z < -z_alpha or Z > z_alpha else \"No\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critical value: 0.025\tz_alpha: 1.960\n",
      "Accuracy: 0.541, n = 257\n",
      "95.0% Confidence interval: [0.480,0.602]\n",
      "Z score: 1.127\n",
      "p-value: 0.12996\n",
      "Critical region corresponding to z_alpha=[-1.960,1.960]: [0.445, 0.567]\n",
      "Can we reject null that 0.541 is different from 0.506 at 95.0 significance level? No\n"
     ]
    }
   ],
   "source": [
    "features=[political_dictionary_feature]\n",
    "predictions, truth=evaluate(trainX, devX, trainY, devY, features)\n",
    "binomial_test(predictions, truth, baseline, significance_level=.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critical value: 0.025\tz_alpha: 1.960\n",
      "Accuracy: 0.610, n = 5000\n",
      "95.0% Confidence interval: [0.596,0.623]\n",
      "Z score: 15.886\n",
      "p-value: 0.00000\n",
      "Critical region corresponding to z_alpha=[-1.960,1.960]: [0.486, 0.514]\n",
      "Can we reject null that 0.610 is different from 0.500 at 95.0 significance level? Yes\n"
     ]
    }
   ],
   "source": [
    "features=[sentiment_dictionary_feature]\n",
    "predictions, truth=evaluate(trainX, devX, trainY, devY, features)\n",
    "binomial_test(predictions, truth, baseline, significance_level=.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critical value: 0.025\tz_alpha: 1.960\n",
      "Accuracy: 0.598, n = 5000\n",
      "95.0% Confidence interval: [0.584,0.611]\n",
      "Z score: 14.073\n",
      "p-value: 0.00000\n",
      "Critical region corresponding to z_alpha=[-1.960,1.960]: [0.486, 0.514]\n",
      "Can we reject null that 0.598 is different from 0.500 at 95.0 significance level? Yes\n"
     ]
    }
   ],
   "source": [
    "features=[new_feature_class_two]\n",
    "predictions, truth=evaluate(trainX, devX, trainY, devY, features)\n",
    "binomial_test(predictions, truth, baseline, significance_level=.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Now apply this same method to the models you just submitted for the last homework.  Are the features you created significantly better than a majority class baseline? Is one significantly better than the other?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
